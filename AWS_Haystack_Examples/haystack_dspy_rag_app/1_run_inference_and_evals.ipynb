{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Haystack pipeline from Amazon S3 and run inferences - Part 2\n",
    "---\n",
    "\n",
    "In this notebook, we will we load the haystack pipeline that is stored in S3 as a `yml` file. Once the haystack pipeline is loaded, we will run a series of questions against the pipeline and measure different metrics, such as latency, accuracy metrics, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "import logging\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import globals as g\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the pipeline from the s3 bucket where it was saved as a `yml` file, and then see the contents of the pipeline. We will then run a series of inference requests against the pipeline and measure latency and accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-15 10:12:35,245] p89542 {credentials.py:1278} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "[2024-11-15 10:12:36,985] p89542 {2606389201.py:12} INFO - Downloaded the haystack pipeline from sagemaker-us-east-2-015469603702/pipelines/basic_rag_pipeline.yml to /var/folders/jy/g9mb5j5n6c11fgdj788p5rww0000gr/T/tmpzcl4cc9d.yml\n"
     ]
    }
   ],
   "source": [
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create temporary file and download pipeline\n",
    "with tempfile.NamedTemporaryFile(mode='wb', suffix='.yml', delete=False) as tmp_file:\n",
    "    # Download from the same location where we uploaded\n",
    "    s3_client.download_fileobj(\n",
    "        g.HAYSTACK_PIPELINE_BUCKET, \n",
    "        g.HAYSTACK_PIPELINE_KEY, \n",
    "        tmp_file\n",
    "    )\n",
    "    logger.info(f\"Downloaded the haystack pipeline from {g.HAYSTACK_PIPELINE_BUCKET}/{g.HAYSTACK_PIPELINE_KEY} to {tmp_file.name}\")\n",
    "    tmp_file_path = tmp_file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xae in position 83: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Now we will load the pipeline from the temporary file path\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tmp_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 4\u001b[0m     loaded_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded the haystack pipeline from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/haystack/core/pipeline/base.py:276\u001b[0m, in \u001b[0;36mPipelineBase.load\u001b[0;34m(cls, fp, marshaller, callbacks)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[T],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     callbacks: Optional[DeserializationCallbacks] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    Creates a `Pipeline` object a string representation.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m        A `Pipeline` object.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mloads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, marshaller, callbacks)\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xae in position 83: invalid start byte"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "# Now we will load the pipeline from the temporary file path\n",
    "with open(tmp_file_path, 'r') as file:\n",
    "    loaded_pipeline = Pipeline.load(file)\n",
    "    print(f\"Loaded the haystack pipeline from {tmp_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-15 10:05:02,592] p87247 {1922383777.py:4} INFO - Loaded Pipeline Structure:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we will clean up the temporary file path and then see the contents of the pipeline\n",
    "# that we had saved to s3\n",
    "os.remove(tmp_file_path)\n",
    "logger.info(\"Loaded Pipeline Structure:\")\n",
    "loaded_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents stored earlier and get those documents in the doc store\n",
    "import json\n",
    "import shutil\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "\n",
    "doc_store_key = \"pipelines/document_store/chroma_store.zip\"\n",
    "chroma_persist_path = os.path.join(g.DATA_DIR, \"chroma_store\")\n",
    "\n",
    "# Later, to download and restore the document store:\n",
    "def load_document_store():\n",
    "    # Download the zip file\n",
    "    chroma_zip_path = os.path.join(g.DATA_DIR, \"chroma_store.zip\")\n",
    "    s3_client.download_file(\n",
    "        g.HAYSTACK_PIPELINE_BUCKET,\n",
    "        doc_store_key,\n",
    "        chroma_zip_path\n",
    "    )\n",
    "    \n",
    "    # Remove existing chroma store if it exists\n",
    "    if os.path.exists(chroma_persist_path):\n",
    "        shutil.rmtree(chroma_persist_path)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    shutil.unpack_archive(chroma_zip_path, g.DATA_DIR, 'zip')\n",
    "    \n",
    "    # Clean up the zip file\n",
    "    os.remove(chroma_zip_path)\n",
    "    \n",
    "    # Initialize ChromaDB with the restored data\n",
    "    return ChromaDocumentStore(\n",
    "        persist_path=chroma_persist_path,\n",
    "        collection_name=\"documents\",\n",
    "        distance_function=\"cosine\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack_integrations.document_stores.chroma.document_store.ChromaDocumentStore at 0x156bddb90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_document_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Integrate the document store into your pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_pipeline\u001b[49m\u001b[38;5;241m.\u001b[39mget_component(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m retriever\u001b[38;5;241m.\u001b[39mdocument_store \u001b[38;5;241m=\u001b[39m load_document_store\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Integrate the document store into your pipeline\n",
    "retriever = loaded_pipeline.get_component('retriever')\n",
    "retriever.document_store = load_document_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inferences against the haystack pipeline using Amazon Bedrock\n",
    "---\n",
    "\n",
    "Now that we have loaded the pipeline from `s3`, we can run some inferences against this RAG pipeline. As we run inferences, we will measure latency and semantic similarity using the `SASEvaluator` module from haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do cC and CXC chemokine levels in children wit...</td>\n",
       "      <td>Chemokines are a superfamily of small peptides...</td>\n",
       "      <td>Initial-phase serum levels of chemokines in pa...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are sAH gene variants associated with obesity-...</td>\n",
       "      <td>The SAH gene locus has recently been proposed ...</td>\n",
       "      <td>We confirm recent evidence that the SAH locus ...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do the functional anatomy of gaze-evoked tinni...</td>\n",
       "      <td>To identify neural sites associated with gaze-...</td>\n",
       "      <td>Patients with GET have plastic changes in mult...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does neighborhood fast-food outlet exposure am...</td>\n",
       "      <td>Greater exposures to fast-food outlets and low...</td>\n",
       "      <td>These findings suggest that efforts to improve...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does conditioned pain modulation predict exerc...</td>\n",
       "      <td>Conditioned pain modulation (CPM) is the conce...</td>\n",
       "      <td>CPM was attenuated in older adults, as measure...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Do cC and CXC chemokine levels in children wit...   \n",
       "1  Are sAH gene variants associated with obesity-...   \n",
       "2  Do the functional anatomy of gaze-evoked tinni...   \n",
       "3  Does neighborhood fast-food outlet exposure am...   \n",
       "4  Does conditioned pain modulation predict exerc...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Chemokines are a superfamily of small peptides...   \n",
       "1  The SAH gene locus has recently been proposed ...   \n",
       "2  To identify neural sites associated with gaze-...   \n",
       "3  Greater exposures to fast-food outlets and low...   \n",
       "4  Conditioned pain modulation (CPM) is the conce...   \n",
       "\n",
       "                                            response   category  \n",
       "0  Initial-phase serum levels of chemokines in pa...  closed_qa  \n",
       "1  We confirm recent evidence that the SAH locus ...  closed_qa  \n",
       "2  Patients with GET have plastic changes in mult...  closed_qa  \n",
       "3  These findings suggest that efforts to improve...  closed_qa  \n",
       "4  CPM was attenuated in older adults, as measure...  closed_qa  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset that we processed in the first notebook\n",
    "df = pd.read_csv(os.path.join(g.DATA_DIR, g.PUBMED_QA_CSV_FNAME)) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-15 09:46:19,048] p81842 {posthog.py:22} INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "[2024-11-15 09:46:20,406] p81842 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:46:20,561] p81842 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:46:20,561] p81842 {pipeline.py:75} INFO - Running component llm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: {'documents': []}\n",
      "LLM Response:\n",
      "A neurodegenerative disease is a condition that involves the progressive loss of structure or function of neurons, leading to their death. Examples of neurodegenerative diseases include:\n",
      "\n",
      "- Alzheimer's disease - a progressive brain disorder that leads to memory loss and cognitive decline.\n",
      "- Parkinson's disease - a disorder that affects movement, causing tremors, stiffness, and difficulty with coordination.\n",
      "- Huntington's disease - a genetic disorder that leads to un\n"
     ]
    }
   ],
   "source": [
    "question: str = \"What is a neurodegenerative disease? Give me examples\"\n",
    "document_store = loaded_pipeline.get_component('retriever')\n",
    "# Directly query the document store to simulate what the BM25 retriever would retrieve\n",
    "retrieved_docs = document_store.run(question)  # Use the question as the query\n",
    "\n",
    "# Print the retrieved documents' content or metadata\n",
    "print(f\"Retrieved Documents: {retrieved_docs}\")\n",
    "\n",
    "# Now proceed with the full pipeline run to generate the final response\n",
    "response = loaded_pipeline.run({\n",
    "    \"retriever\": {\"query\": question},\n",
    "    \"prompt_builder\": {\"question\": question}\n",
    "})\n",
    "print(\"LLM Response:\")\n",
    "print(response[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure the `SASEvaluation` score\n",
    "\n",
    "Now, we will use Haystack's `SAS evaluator` to generate answers to the first 20 questions of the dataset. `SASEvaluator` will evaluate the answer predicted my the pipeline that we have loaded and then compare the semantic similarity to the ground truth provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-15 09:23:52,639] p69674 {SentenceTransformer.py:218} INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "[2024-11-15 09:23:57,774] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:23:57,938] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:23:57,938] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:00,333] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:00,437] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:00,438] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:00,448] p69674 {generator.py:182} WARNING - The prompt was truncated from 4513 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:02,368] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:02,451] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:02,451] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:04,802] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:04,907] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:04,908] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:06,728] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:06,832] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:06,833] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:06,843] p69674 {generator.py:182} WARNING - The prompt was truncated from 4474 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:08,095] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:08,212] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:08,213] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:08,223] p69674 {generator.py:182} WARNING - The prompt was truncated from 4223 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:10,116] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:10,232] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:10,233] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:12,400] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:12,517] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:12,518] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:12,529] p69674 {generator.py:182} WARNING - The prompt was truncated from 4581 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:14,274] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:14,379] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:14,380] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:14,389] p69674 {generator.py:182} WARNING - The prompt was truncated from 4807 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:15,557] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:15,661] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:15,662] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:15,670] p69674 {generator.py:182} WARNING - The prompt was truncated from 4205 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:17,041] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:17,168] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:17,169] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:17,178] p69674 {generator.py:182} WARNING - The prompt was truncated from 4139 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:18,941] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:19,043] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:19,043] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:19,052] p69674 {generator.py:182} WARNING - The prompt was truncated from 4290 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:21,038] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:21,135] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:21,135] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:23,507] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:23,624] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:23,624] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:26,003] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:26,109] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:26,109] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:28,011] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:28,114] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:28,114] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:28,123] p69674 {generator.py:182} WARNING - The prompt was truncated from 4479 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:30,669] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:30,793] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:30,793] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:32,690] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:32,827] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:32,828] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:34,314] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:34,431] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:34,432] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:34,445] p69674 {generator.py:182} WARNING - The prompt was truncated from 4771 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:35,590] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:35,707] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:35,707] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:35,717] p69674 {generator.py:182} WARNING - The prompt was truncated from 4478 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:37,364] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:37,484] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:37,485] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:37,495] p69674 {generator.py:182} WARNING - The prompt was truncated from 4277 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:39,188] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:39,316] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:39,316] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:39,326] p69674 {generator.py:182} WARNING - The prompt was truncated from 4631 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:41,724] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:41,838] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:41,838] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:43,895] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:43,993] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:43,993] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:46,340] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:46,439] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:46,440] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:46,448] p69674 {generator.py:182} WARNING - The prompt was truncated from 4057 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:48,033] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:48,135] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:48,135] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:48,145] p69674 {generator.py:182} WARNING - The prompt was truncated from 4223 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:49,855] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:49,955] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:49,956] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:49,965] p69674 {generator.py:182} WARNING - The prompt was truncated from 4081 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:52,271] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:52,375] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:52,376] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:54,370] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:54,470] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:54,471] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:54,480] p69674 {generator.py:182} WARNING - The prompt was truncated from 4094 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:55,723] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:55,818] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:55,818] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:55,827] p69674 {generator.py:182} WARNING - The prompt was truncated from 4060 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:24:58,727] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:24:58,837] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:24:58,837] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:24:58,846] p69674 {generator.py:182} WARNING - The prompt was truncated from 4116 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:00,467] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:00,586] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:00,587] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:00,597] p69674 {generator.py:182} WARNING - The prompt was truncated from 5133 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:02,296] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:02,402] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:02,403] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:04,385] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:04,504] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:04,504] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:07,265] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:07,372] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:07,372] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:07,381] p69674 {generator.py:182} WARNING - The prompt was truncated from 4156 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:09,022] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:09,119] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:09,120] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:09,129] p69674 {generator.py:182} WARNING - The prompt was truncated from 4228 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:11,468] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:11,570] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:11,571] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:11,580] p69674 {generator.py:182} WARNING - The prompt was truncated from 4327 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:14,594] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:14,706] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:14,706] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:16,129] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:16,303] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:16,304] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:16,316] p69674 {generator.py:182} WARNING - The prompt was truncated from 4330 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:18,375] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:18,485] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:18,486] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:20,489] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:20,601] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:20,601] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:20,610] p69674 {generator.py:182} WARNING - The prompt was truncated from 4050 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:22,681] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:22,778] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:22,778] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:24,697] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:24,800] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:24,800] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:24,818] p69674 {generator.py:182} WARNING - The prompt was truncated from 4667 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:26,488] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:26,601] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:26,601] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:26,611] p69674 {generator.py:182} WARNING - The prompt was truncated from 4808 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:29,552] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:29,669] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:29,669] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:29,680] p69674 {generator.py:182} WARNING - The prompt was truncated from 5035 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:31,119] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:31,223] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:31,223] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:33,135] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:33,262] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:33,262] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:34,784] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:34,889] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:34,890] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:34,899] p69674 {generator.py:182} WARNING - The prompt was truncated from 4063 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:36,719] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:36,832] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:36,833] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:36,841] p69674 {generator.py:182} WARNING - The prompt was truncated from 4114 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:39,402] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:39,546] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:39,546] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:39,555] p69674 {generator.py:182} WARNING - The prompt was truncated from 4191 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:42,519] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:42,623] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:42,624] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:42,632] p69674 {generator.py:182} WARNING - The prompt was truncated from 4338 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:45,299] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:45,407] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:45,407] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:45,418] p69674 {generator.py:182} WARNING - The prompt was truncated from 4615 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:47,255] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:47,366] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:47,366] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:49,345] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:49,476] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:49,476] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:51,422] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:51,527] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:51,528] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:51,536] p69674 {generator.py:182} WARNING - The prompt was truncated from 4186 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:25:53,705] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:53,812] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:53,812] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:55,547] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:55,650] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:55,650] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:25:57,066] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:25:57,166] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:25:57,167] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:01,380] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:01,488] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:01,488] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:01,497] p69674 {generator.py:182} WARNING - The prompt was truncated from 4223 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:03,067] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:03,165] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:03,166] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:03,175] p69674 {generator.py:182} WARNING - The prompt was truncated from 4004 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:05,065] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:05,174] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:05,175] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:05,183] p69674 {generator.py:182} WARNING - The prompt was truncated from 4010 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:07,092] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:07,197] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:07,197] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:07,206] p69674 {generator.py:182} WARNING - The prompt was truncated from 4479 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:09,050] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:09,199] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:09,199] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:09,209] p69674 {generator.py:182} WARNING - The prompt was truncated from 4075 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:10,713] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:10,819] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:10,820] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:13,158] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:13,260] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:13,260] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:13,270] p69674 {generator.py:182} WARNING - The prompt was truncated from 4679 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:15,529] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:15,644] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:15,644] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:15,653] p69674 {generator.py:182} WARNING - The prompt was truncated from 4345 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:16,998] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:17,096] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:17,097] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:17,106] p69674 {generator.py:182} WARNING - The prompt was truncated from 4197 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:18,579] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:18,677] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:18,677] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:21,616] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:21,718] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:21,718] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:21,727] p69674 {generator.py:182} WARNING - The prompt was truncated from 4273 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:23,788] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:23,905] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:23,906] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:25,812] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:25,911] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:25,911] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:25,921] p69674 {generator.py:182} WARNING - The prompt was truncated from 4695 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:27,889] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:27,988] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:27,989] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:27,999] p69674 {generator.py:182} WARNING - The prompt was truncated from 4596 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:29,623] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:29,731] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:29,732] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:29,740] p69674 {generator.py:182} WARNING - The prompt was truncated from 4013 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:31,920] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:32,028] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:32,028] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:32,037] p69674 {generator.py:182} WARNING - The prompt was truncated from 4432 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:34,610] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:34,710] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:34,711] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:34,720] p69674 {generator.py:182} WARNING - The prompt was truncated from 4344 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:36,901] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:37,003] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:37,003] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:40,073] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:40,173] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:40,173] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:40,183] p69674 {generator.py:182} WARNING - The prompt was truncated from 4513 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:41,705] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:41,800] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:41,800] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:43,176] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:43,275] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:43,275] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:43,285] p69674 {generator.py:182} WARNING - The prompt was truncated from 4728 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:45,086] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:45,188] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:45,188] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:47,463] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:47,590] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:47,591] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:49,709] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:49,819] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:49,819] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:49,828] p69674 {generator.py:182} WARNING - The prompt was truncated from 4432 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:51,577] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:51,661] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:51,661] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:51,670] p69674 {generator.py:182} WARNING - The prompt was truncated from 4230 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:53,413] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:53,521] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:53,521] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:53,531] p69674 {generator.py:182} WARNING - The prompt was truncated from 4170 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:55,486] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:55,601] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:55,602] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:55,611] p69674 {generator.py:182} WARNING - The prompt was truncated from 4053 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:26:57,891] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:26:57,993] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:26:57,994] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:26:58,004] p69674 {generator.py:182} WARNING - The prompt was truncated from 4235 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:00,131] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:00,235] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:00,235] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:00,244] p69674 {generator.py:182} WARNING - The prompt was truncated from 4193 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:02,398] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:02,504] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:02,505] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:02,515] p69674 {generator.py:182} WARNING - The prompt was truncated from 4867 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:04,550] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:04,655] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:04,655] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:04,695] p69674 {generator.py:182} WARNING - The prompt was truncated from 4696 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:06,341] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:06,438] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:06,438] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:08,137] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:08,240] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:08,240] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:09,666] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:09,781] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:09,782] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:12,395] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:12,519] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:12,519] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:14,099] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:14,201] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:14,201] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:14,210] p69674 {generator.py:182} WARNING - The prompt was truncated from 4421 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:15,924] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:16,023] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:16,023] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:16,033] p69674 {generator.py:182} WARNING - The prompt was truncated from 4018 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:17,445] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:17,551] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:17,551] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:19,603] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:19,710] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:19,711] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:19,721] p69674 {generator.py:182} WARNING - The prompt was truncated from 4979 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:21,235] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:21,342] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:21,342] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:23,645] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:23,754] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:23,755] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:25,771] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:25,869] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:25,869] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:25,878] p69674 {generator.py:182} WARNING - The prompt was truncated from 4202 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:28,537] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:28,637] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:28,637] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:30,191] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:30,291] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:30,292] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:32,226] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:32,350] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:32,351] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:32,361] p69674 {generator.py:182} WARNING - The prompt was truncated from 4113 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:34,284] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:34,422] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:34,423] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:36,903] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:37,058] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:37,059] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:38,796] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:38,901] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:38,902] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:38,913] p69674 {generator.py:182} WARNING - The prompt was truncated from 4459 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:40,721] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:40,841] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:40,841] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:42,302] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:42,402] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:42,402] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:42,411] p69674 {generator.py:182} WARNING - The prompt was truncated from 4256 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:44,661] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:44,768] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:44,769] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:44,777] p69674 {generator.py:182} WARNING - The prompt was truncated from 4093 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:46,598] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:46,730] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:46,731] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:48,719] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:48,824] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:48,824] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:51,085] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:51,218] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:51,219] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:51,230] p69674 {generator.py:182} WARNING - The prompt was truncated from 4521 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:27:52,709] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:52,837] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:52,837] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:54,745] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:54,849] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:54,849] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:56,451] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:56,569] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:56,569] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:58,900] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:27:59,015] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:27:59,016] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:27:59,026] p69674 {generator.py:182} WARNING - The prompt was truncated from 4464 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:01,088] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:01,184] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:01,184] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:01,193] p69674 {generator.py:182} WARNING - The prompt was truncated from 4168 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:02,772] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:02,882] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:02,883] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:04,816] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:04,915] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:04,916] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:04,926] p69674 {generator.py:182} WARNING - The prompt was truncated from 4350 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:06,432] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:06,532] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:06,533] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:06,542] p69674 {generator.py:182} WARNING - The prompt was truncated from 4324 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:08,920] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:09,026] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:09,027] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:09,036] p69674 {generator.py:182} WARNING - The prompt was truncated from 4369 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:10,949] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:11,059] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:11,059] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:11,069] p69674 {generator.py:182} WARNING - The prompt was truncated from 4457 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:13,471] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:13,604] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:13,604] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:15,730] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:15,835] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:15,835] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:15,845] p69674 {generator.py:182} WARNING - The prompt was truncated from 4592 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:18,323] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:18,429] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:18,430] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:20,350] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:20,450] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:20,450] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:22,199] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:22,297] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:22,298] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:23,861] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:23,967] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:23,968] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:26,390] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:26,489] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:26,490] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:26,501] p69674 {generator.py:182} WARNING - The prompt was truncated from 4100 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:28,908] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:29,017] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:29,018] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:30,943] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:31,061] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:31,062] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:31,101] p69674 {generator.py:182} WARNING - The prompt was truncated from 4037 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:32,739] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:32,864] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:32,865] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:32,874] p69674 {generator.py:182} WARNING - The prompt was truncated from 4030 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:34,747] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:34,866] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:34,866] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:36,354] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:36,465] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:36,465] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:36,475] p69674 {generator.py:182} WARNING - The prompt was truncated from 4088 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:38,987] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:39,104] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:39,105] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:39,114] p69674 {generator.py:182} WARNING - The prompt was truncated from 4299 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:40,541] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:40,649] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:40,650] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:40,659] p69674 {generator.py:182} WARNING - The prompt was truncated from 4391 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:42,761] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:42,878] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:42,878] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:42,887] p69674 {generator.py:182} WARNING - The prompt was truncated from 4128 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:45,540] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:45,681] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:45,682] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:47,898] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:48,022] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:48,022] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:49,391] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:49,596] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:49,596] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:52,190] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:52,290] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:52,290] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:54,374] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:54,487] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:54,487] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:54,497] p69674 {generator.py:182} WARNING - The prompt was truncated from 4023 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:56,280] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:56,393] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:56,394] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:56,403] p69674 {generator.py:182} WARNING - The prompt was truncated from 4146 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:28:58,158] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:28:58,269] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:28:58,269] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:28:58,279] p69674 {generator.py:182} WARNING - The prompt was truncated from 4547 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:00,137] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:00,478] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:00,480] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:00,500] p69674 {generator.py:182} WARNING - The prompt was truncated from 4434 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:02,654] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:02,766] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:02,766] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:02,775] p69674 {generator.py:182} WARNING - The prompt was truncated from 4068 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:04,944] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:05,055] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:05,055] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:06,796] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:06,910] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:06,910] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:06,920] p69674 {generator.py:182} WARNING - The prompt was truncated from 4405 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:09,191] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:09,302] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:09,302] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:09,311] p69674 {generator.py:182} WARNING - The prompt was truncated from 4183 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:11,301] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:11,404] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:11,405] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:11,414] p69674 {generator.py:182} WARNING - The prompt was truncated from 4077 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:13,361] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:13,463] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:13,464] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:15,732] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:15,847] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:15,848] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:15,857] p69674 {generator.py:182} WARNING - The prompt was truncated from 4049 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:17,734] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:17,837] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:17,838] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:17,847] p69674 {generator.py:182} WARNING - The prompt was truncated from 4177 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:19,676] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:19,776] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:19,777] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:19,786] p69674 {generator.py:182} WARNING - The prompt was truncated from 4285 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:21,807] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:21,915] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:21,916] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:21,925] p69674 {generator.py:182} WARNING - The prompt was truncated from 4548 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:23,447] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:23,551] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:23,552] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:23,561] p69674 {generator.py:182} WARNING - The prompt was truncated from 4516 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:25,734] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:25,834] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:25,834] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:25,845] p69674 {generator.py:182} WARNING - The prompt was truncated from 4905 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:27,447] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:27,581] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:27,581] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:27,591] p69674 {generator.py:182} WARNING - The prompt was truncated from 4011 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:30,157] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:30,292] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:30,293] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:30,302] p69674 {generator.py:182} WARNING - The prompt was truncated from 4058 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:32,308] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:32,429] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:32,430] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:32,440] p69674 {generator.py:182} WARNING - The prompt was truncated from 4839 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:34,244] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:34,360] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:34,361] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:34,370] p69674 {generator.py:182} WARNING - The prompt was truncated from 4086 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:36,570] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:36,674] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:36,674] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:36,683] p69674 {generator.py:182} WARNING - The prompt was truncated from 4539 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:39,269] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:39,364] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:39,364] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:40,857] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:41,038] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:41,039] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:43,387] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:43,519] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:43,519] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:43,530] p69674 {generator.py:182} WARNING - The prompt was truncated from 4284 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:45,414] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:45,538] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:45,538] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:45,548] p69674 {generator.py:182} WARNING - The prompt was truncated from 4436 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:46,915] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:47,022] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:47,022] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:47,032] p69674 {generator.py:182} WARNING - The prompt was truncated from 4760 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:48,384] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:48,529] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:48,529] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:51,454] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:51,612] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:51,613] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:51,644] p69674 {generator.py:182} WARNING - The prompt was truncated from 4341 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:53,883] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:54,016] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:54,016] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:54,025] p69674 {generator.py:182} WARNING - The prompt was truncated from 4296 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:29:55,798] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:55,906] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:55,906] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:29:58,293] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:29:58,401] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:29:58,402] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:01,063] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:01,162] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:01,162] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:03,208] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:03,316] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:03,316] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:03,325] p69674 {generator.py:182} WARNING - The prompt was truncated from 4582 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:05,385] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:05,507] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:05,507] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:05,518] p69674 {generator.py:182} WARNING - The prompt was truncated from 4860 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:07,170] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:07,272] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:07,273] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:07,282] p69674 {generator.py:182} WARNING - The prompt was truncated from 4503 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:09,062] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:09,297] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:09,298] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:09,310] p69674 {generator.py:182} WARNING - The prompt was truncated from 4292 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:10,890] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:10,998] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:10,999] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:13,926] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:14,057] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:14,057] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:14,067] p69674 {generator.py:182} WARNING - The prompt was truncated from 4202 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:15,782] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:15,908] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:15,909] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:18,442] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:18,545] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:18,545] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:18,555] p69674 {generator.py:182} WARNING - The prompt was truncated from 4497 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:19,958] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:20,062] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:20,062] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:20,071] p69674 {generator.py:182} WARNING - The prompt was truncated from 4400 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:22,424] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:22,601] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:22,601] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:22,612] p69674 {generator.py:182} WARNING - The prompt was truncated from 4447 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:24,589] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:24,839] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:24,841] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:27,572] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:27,684] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:27,684] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:27,695] p69674 {generator.py:182} WARNING - The prompt was truncated from 4280 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:29,323] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:29,439] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:29,439] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:31,097] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:31,231] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:31,232] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:33,533] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:33,633] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:33,633] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:33,643] p69674 {generator.py:182} WARNING - The prompt was truncated from 4003 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:35,958] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:36,072] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:36,072] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:38,897] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:39,013] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:39,013] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:39,022] p69674 {generator.py:182} WARNING - The prompt was truncated from 4346 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:41,453] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:41,553] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:41,553] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:41,562] p69674 {generator.py:182} WARNING - The prompt was truncated from 4476 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:43,524] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:43,630] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:43,631] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:44,974] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:45,073] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:45,073] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:47,473] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:47,583] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:47,583] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:47,594] p69674 {generator.py:182} WARNING - The prompt was truncated from 4410 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:49,078] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:49,182] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:49,183] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:49,191] p69674 {generator.py:182} WARNING - The prompt was truncated from 4144 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:51,037] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:51,139] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:51,140] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:51,148] p69674 {generator.py:182} WARNING - The prompt was truncated from 4175 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:53,283] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:53,392] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:53,393] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:53,402] p69674 {generator.py:182} WARNING - The prompt was truncated from 4698 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:54,938] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:55,053] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:55,053] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:55,063] p69674 {generator.py:182} WARNING - The prompt was truncated from 4587 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:30:56,484] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:56,581] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:56,582] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:58,748] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:30:58,876] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:30:58,877] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:30:58,886] p69674 {generator.py:182} WARNING - The prompt was truncated from 4638 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:00,589] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:00,696] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:00,697] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:00,707] p69674 {generator.py:182} WARNING - The prompt was truncated from 4430 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:03,024] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:03,120] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:03,121] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:03,129] p69674 {generator.py:182} WARNING - The prompt was truncated from 4046 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:04,746] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:04,874] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:04,875] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:04,884] p69674 {generator.py:182} WARNING - The prompt was truncated from 4031 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:06,969] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:07,103] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:07,103] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:09,666] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:09,820] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:09,821] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:11,215] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:11,343] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:11,343] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:11,352] p69674 {generator.py:182} WARNING - The prompt was truncated from 4274 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:13,424] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:13,552] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:13,552] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:13,562] p69674 {generator.py:182} WARNING - The prompt was truncated from 4209 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:15,768] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:15,865] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:15,865] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:15,875] p69674 {generator.py:182} WARNING - The prompt was truncated from 4076 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:18,304] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:18,398] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:18,398] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:18,407] p69674 {generator.py:182} WARNING - The prompt was truncated from 4037 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:19,968] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:20,077] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:20,077] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:22,042] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:22,153] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:22,153] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:22,164] p69674 {generator.py:182} WARNING - The prompt was truncated from 4145 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:23,613] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:23,736] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:23,736] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:25,056] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:25,166] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:25,166] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:25,175] p69674 {generator.py:182} WARNING - The prompt was truncated from 4092 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:28,111] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:28,219] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:28,219] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:28,230] p69674 {generator.py:182} WARNING - The prompt was truncated from 4932 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:30,432] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:30,600] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:30,601] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:33,485] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:33,591] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:33,591] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:35,322] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:35,425] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:35,426] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:37,359] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:37,463] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:37,464] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:38,901] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:39,011] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:39,012] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:39,021] p69674 {generator.py:182} WARNING - The prompt was truncated from 4422 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:40,640] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:40,846] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:40,847] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:40,857] p69674 {generator.py:182} WARNING - The prompt was truncated from 4027 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:43,420] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:43,521] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:43,521] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:43,531] p69674 {generator.py:182} WARNING - The prompt was truncated from 4551 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:45,245] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:45,346] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:45,347] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:45,357] p69674 {generator.py:182} WARNING - The prompt was truncated from 3998 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:48,094] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:48,199] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:48,199] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:48,210] p69674 {generator.py:182} WARNING - The prompt was truncated from 4731 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:50,296] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:50,414] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:50,415] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:50,424] p69674 {generator.py:182} WARNING - The prompt was truncated from 4125 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:52,195] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:52,351] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:52,352] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:52,417] p69674 {generator.py:182} WARNING - The prompt was truncated from 4286 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n",
      "[2024-11-15 09:31:54,738] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:54,851] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:54,851] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:56,888] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:57,002] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:57,002] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:31:58,907] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:31:59,015] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:31:59,015] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:32:00,585] p69674 {pipeline.py:75} INFO - Running component retriever\n",
      "[2024-11-15 09:32:00,705] p69674 {pipeline.py:75} INFO - Running component prompt_builder\n",
      "[2024-11-15 09:32:00,705] p69674 {pipeline.py:75} INFO - Running component llm\n",
      "[2024-11-15 09:32:00,715] p69674 {generator.py:182} WARNING - The prompt was truncated from 4246 tokens to 3996 tokens so that the prompt length and the answer length (100 tokens) fit within the model's max token limit (4096 tokens). Shorten the prompt or it will be cut off.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m test_queries:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Run the pipeline to get the predicted answer\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mretriever\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_builder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     predicted_answer \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplies\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(predicted_answer)\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/haystack/core/pipeline/pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data, include_outputs_from)\u001b[0m\n\u001b[1;32m    468\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum run count \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_runs_per_component\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached for component \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineMaxComponentRuns(msg)\n\u001b[0;32m--> 471\u001b[0m res: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponents_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# Delete the inputs that were consumed by the Component and are not received from the user\u001b[39;00m\n\u001b[1;32m    474\u001b[0m sockets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(components_inputs[name]\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/haystack/core/pipeline/pipeline.py:76\u001b[0m, in \u001b[0;36mPipeline._run_component\u001b[0;34m(self, name, inputs, parent_span)\u001b[0m\n\u001b[1;32m     74\u001b[0m span\u001b[38;5;241m.\u001b[39mset_content_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaystack.component.input\u001b[39m\u001b[38;5;124m\"\u001b[39m, deepcopy(inputs))\n\u001b[1;32m     75\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning component \u001b[39m\u001b[38;5;132;01m{component_name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, component_name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m---> 76\u001b[0m res: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# After a Component that has variadic inputs is run, we need to reset the variadic inputs that were consumed\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/haystack_integrations/components/generators/amazon_bedrock/generator.py:234\u001b[0m, in \u001b[0;36mAmazonBedrockGenerator.run\u001b[0;34m(self, prompt, streaming_callback, generation_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     replies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_adapter\u001b[38;5;241m.\u001b[39mget_stream_responses(\n\u001b[1;32m    231\u001b[0m         stream\u001b[38;5;241m=\u001b[39mresponse_stream, streaming_callback\u001b[38;5;241m=\u001b[39mstreaming_callback\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     response_body \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    241\u001b[0m     replies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_adapter\u001b[38;5;241m.\u001b[39mget_responses(response_body\u001b[38;5;241m=\u001b[39mresponse_body)\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/client.py:999\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    995\u001b[0m     maybe_compress_request(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[1;32m    997\u001b[0m     )\n\u001b[1;32m    998\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m--> 999\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1005\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1009\u001b[0m )\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1027\u001b[0m             exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   1028\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1029\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/endpoint.py:197\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m    196\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[0;32m--> 197\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    201\u001b[0m     attempts,\n\u001b[1;32m    202\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     exception,\n\u001b[1;32m    206\u001b[0m ):\n\u001b[1;32m    207\u001b[0m     attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/endpoint.py:239\u001b[0m, in \u001b[0;36mEndpoint._get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, operation_model, context):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# This will return a tuple of (success_response, exception)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# and success_response is itself a tuple of\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# (http_response, parsed_dict).\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# If an exception occurs then the success_response is None.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# If no exception occurs then exception is None.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     kwargs_to_emit \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_response\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: context,\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m'\u001b[39m: exception,\n\u001b[1;32m    247\u001b[0m     }\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/endpoint.py:279\u001b[0m, in \u001b[0;36mEndpoint._do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    277\u001b[0m     http_response \u001b[38;5;241m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m http_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         http_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, e)\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/endpoint.py:375\u001b[0m, in \u001b[0;36mEndpoint._send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/botocore/httpsession.py:464\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    461\u001b[0m     conn\u001b[38;5;241m.\u001b[39mproxy_headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m host\n\u001b[1;32m    463\u001b[0m request_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_target(request\u001b[38;5;241m.\u001b[39murl, proxy_url)\n\u001b[0;32m--> 464\u001b[0m urllib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m http_response \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mawsrequest\u001b[38;5;241m.\u001b[39mAWSResponse(\n\u001b[1;32m    477\u001b[0m     request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    478\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    479\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    480\u001b[0m     urllib_response,\n\u001b[1;32m    481\u001b[0m )\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request\u001b[38;5;241m.\u001b[39mstream_output:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# Cause the raw stream to be exhausted immediately. We do it\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# this way instead of using preload_content because\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# preload_content will never buffer chunked responses\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1386\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from haystack.components.evaluators import SASEvaluator\n",
    "\n",
    "# Extract the questions and ground truth answers\n",
    "test_queries = df['instruction'].tolist()\n",
    "ground_truth_answers = df['response'].tolist()\n",
    "\n",
    "# Initialize the SAS evaluator\n",
    "sas_evaluator = SASEvaluator()\n",
    "sas_evaluator.warm_up()\n",
    "\n",
    "# Running the pipeline to get predictions\n",
    "results = []\n",
    "for question in test_queries:\n",
    "    # Run the pipeline to get the predicted answer\n",
    "    response = loaded_pipeline.run({\n",
    "        \"retriever\": {\"query\": question},\n",
    "        \"prompt_builder\": {\"question\": question}\n",
    "    })\n",
    "    predicted_answer = response[\"llm\"][\"replies\"][0]\n",
    "    results.append(predicted_answer)\n",
    "\n",
    "# Evaluate the results using SAS evaluator\n",
    "sas_evaluation = sas_evaluator.run(ground_truth_answers=ground_truth_answers, predicted_answers=results)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Semantic Answer Similarity Evaluation:\")\n",
    "print(\"Individual scores:\", sas_evaluation[\"individual_scores\"])\n",
    "print(\"Overall mean SAS score:\", sas_evaluation[\"score\"])\n",
    "\n",
    "# Optionally, print detailed results\n",
    "print(\"\\nDetailed Results:\")\n",
    "for i, (query, ground_truth, predicted) in enumerate(zip(test_queries, ground_truth_answers, results)):\n",
    "    print(f\"\\nQuery {i+1}: {query}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(f\"SAS Score: {sas_evaluation['individual_scores'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Head:\n",
      "   query_id                                              query  \\\n",
      "0         1  Do cC and CXC chemokine levels in children wit...   \n",
      "1         2  Are sAH gene variants associated with obesity-...   \n",
      "2         3  Do the functional anatomy of gaze-evoked tinni...   \n",
      "3         4  Does neighborhood fast-food outlet exposure am...   \n",
      "4         5  Does conditioned pain modulation predict exerc...   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0  Initial-phase serum levels of chemokines in pa...   \n",
      "1  We confirm recent evidence that the SAH locus ...   \n",
      "2  Patients with GET have plastic changes in mult...   \n",
      "3  These findings suggest that efforts to improve...   \n",
      "4  CPM was attenuated in older adults, as measure...   \n",
      "\n",
      "                                    predicted_answer  sas_score  \n",
      "0  Based on the context provided, the key finding...   0.829714  \n",
      "1  Here are the key points from the summarized re...   0.778862  \n",
      "2  The key findings regarding the functional anat...   0.728487  \n",
      "3  Yes, the findings suggest that greater exposur...   0.763646  \n",
      "4  Based on the information provided, the key fin...   0.275946  \n",
      "\n",
      "Overall Mean SAS Score: 0.7092914409935475\n",
      "\n",
      "Results saved to data/evaluation_results_before_dspy.csv\n"
     ]
    }
   ],
   "source": [
    "# save the evaluation metrics to a csv file\n",
    "evaluation_data = []\n",
    "for i, (query, ground_truth, predicted, score) in enumerate(zip(\n",
    "    test_queries, \n",
    "    ground_truth_answers, \n",
    "    results, \n",
    "    sas_evaluation['individual_scores']\n",
    ")):\n",
    "    evaluation_data.append({\n",
    "        'query_id': i + 1,\n",
    "        'query': query,\n",
    "        'ground_truth': ground_truth,\n",
    "        'predicted_answer': predicted,\n",
    "        'sas_score': score\n",
    "    })\n",
    "df_evaluation = pd.DataFrame(evaluation_data)\n",
    "\n",
    "# Add the overall mean score as metadata\n",
    "df_evaluation.attrs['mean_sas_score'] = sas_evaluation['score']\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame Head:\")\n",
    "print(df_evaluation.head())\n",
    "print(f\"\\nOverall Mean SAS Score: {df_evaluation.attrs['mean_sas_score']}\")\n",
    "\n",
    "# Save to CSV\n",
    "csv_fpath_for_initial_eval = os.path.join(g.DATA_DIR, 'evaluation_results_before_dspy.csv')\n",
    "df_evaluation.to_csv(csv_fpath_for_initial_eval, index=False)\n",
    "print(f\"\\nResults saved to {csv_fpath_for_initial_eval}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
