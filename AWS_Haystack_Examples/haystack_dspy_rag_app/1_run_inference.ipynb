{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Haystack pipeline from Amazon S3 and run inferences - Part 2\n",
    "---\n",
    "\n",
    "In this notebook, we will we load the haystack pipeline that is stored in S3 as a `yml` file. Once the haystack pipeline is loaded, we will run a series of questions against the pipeline and measure different metrics, such as latency, accuracy metrics, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "import logging\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import globals as g\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the pipeline from the s3 bucket where it was saved as a `yml` file, and then see the contents of the pipeline. We will then run a series of inference requests against the pipeline and measure latency and accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create temporary file and download pipeline\n",
    "with tempfile.NamedTemporaryFile(mode='wb', suffix='.yml', delete=False) as tmp_file:\n",
    "    # Download from the same location where we uploaded\n",
    "    s3_client.download_fileobj(\n",
    "        g.HAYSTACK_PIPELINE_BUCKET, \n",
    "        g.HAYSTACK_PIPELINE_KEY, \n",
    "        tmp_file\n",
    "    )\n",
    "    logger.info(f\"Downloaded the haystack pipeline from {g.HAYSTACK_PIPELINE_BUCKET}/{g.HAYSTACK_PIPELINE_KEY} to {tmp_file.name}\")\n",
    "    tmp_file_path = tmp_file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load document store\n",
    "doc_store_key = \"pipelines/document_store/documents.json\"\n",
    "with tempfile.NamedTemporaryFile(mode='wb', suffix='.pkl', delete=False) as tmp_file:\n",
    "    s3_client.download_fileobj(\n",
    "        g.HAYSTACK_PIPELINE_BUCKET, \n",
    "        doc_store_key, \n",
    "        tmp_file\n",
    "    )\n",
    "    print(f\"Downloaded the document store from {g.HAYSTACK_PIPELINE_BUCKET}/{doc_store_key} to {tmp_file.name}\")\n",
    "    docstore_tmp_path = tmp_file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "# Now we will load the pipeline from the temporary file path\n",
    "with open(tmp_file_path, 'r') as file:\n",
    "    loaded_pipeline = Pipeline.load(file)\n",
    "    print(f\"Loaded the haystack pipeline from {tmp_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will clean up the temporary file path and then see the contents of the pipeline\n",
    "# that we had saved to s3\n",
    "os.remove(tmp_file_path)\n",
    "logger.info(\"Loaded Pipeline Structure:\")\n",
    "loaded_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents stored earlier and get those documents in the doc store\n",
    "import json\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "\n",
    "# Load the documents from the JSON file\n",
    "with open(docstore_tmp_path, 'r') as f:\n",
    "    documents_dicts = json.load(f)\n",
    "\n",
    "from haystack import Document\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings back to NumPy arrays\n",
    "for doc_dict in documents_dicts:\n",
    "    if 'embedding' in doc_dict and isinstance(doc_dict['embedding'], list):\n",
    "        doc_dict['embedding'] = np.array(doc_dict['embedding'])\n",
    "\n",
    "# Reconstruct Document objects\n",
    "documents = [Document.from_dict(doc_dict) for doc_dict in documents_dicts]\n",
    "document_store = ChromaDocumentStore()\n",
    "# Write the documents to the document store\n",
    "document_store.write_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate the document store into your pipeline\n",
    "retriever = loaded_pipeline.get_component('retriever')\n",
    "retriever.document_store = document_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inferences against the haystack pipeline using Amazon Bedrock\n",
    "---\n",
    "\n",
    "Now that we have loaded the pipeline from `s3`, we can run some inferences against this RAG pipeline. As we run inferences, we will measure latency and semantic similarity using the `SASEvaluator` module from haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset that we processed in the first notebook\n",
    "df = pd.read_csv(os.path.join(g.DATA_DIR, g.PUBMED_QA_CSV_FNAME)) \n",
    "\n",
    "# We will select the first 10 rows of the dataset for testing\n",
    "test_data = df.head(20)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question: str = \"What is a neurodegenerative disease? Give me examples\"\n",
    "document_store = loaded_pipeline.get_component('retriever')\n",
    "# Directly query the document store to simulate what the BM25 retriever would retrieve\n",
    "retrieved_docs = document_store.run(question)  # Use the question as the query\n",
    "\n",
    "# Print the retrieved documents' content or metadata\n",
    "print(f\"Retrieved Documents: {retrieved_docs}\")\n",
    "\n",
    "# Now proceed with the full pipeline run to generate the final response\n",
    "response = loaded_pipeline.run({\n",
    "    \"retriever\": {\"query\": question},\n",
    "    \"prompt_builder\": {\"question\": question}\n",
    "})\n",
    "print(\"LLM Response:\")\n",
    "print(response[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure the `SASEvaluation` score\n",
    "\n",
    "Now, we will use Haystack's `SAS evaluator` to generate answers to the first 20 questions of the dataset. `SASEvaluator` will evaluate the answer predicted my the pipeline that we have loaded and then compare the semantic similarity to the ground truth provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.evaluators import SASEvaluator\n",
    "\n",
    "# Extract the questions and ground truth answers\n",
    "test_queries = test_data['instruction'].tolist()\n",
    "ground_truth_answers = test_data['response'].tolist()\n",
    "\n",
    "# Initialize the SAS evaluator\n",
    "sas_evaluator = SASEvaluator()\n",
    "sas_evaluator.warm_up()\n",
    "\n",
    "# Running the pipeline to get predictions\n",
    "results = []\n",
    "for question in test_queries:\n",
    "    # Run the pipeline to get the predicted answer\n",
    "    response = loaded_pipeline.run({\n",
    "        \"retriever\": {\"query\": question},\n",
    "        \"prompt_builder\": {\"question\": question}\n",
    "    })\n",
    "    predicted_answer = response[\"llm\"][\"replies\"][0]\n",
    "    results.append(predicted_answer)\n",
    "\n",
    "# Evaluate the results using SAS evaluator\n",
    "sas_evaluation = sas_evaluator.run(ground_truth_answers=ground_truth_answers, predicted_answers=results)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Semantic Answer Similarity Evaluation:\")\n",
    "print(\"Individual scores:\", sas_evaluation[\"individual_scores\"])\n",
    "print(\"Overall mean SAS score:\", sas_evaluation[\"score\"])\n",
    "\n",
    "# Optionally, print detailed results\n",
    "print(\"\\nDetailed Results:\")\n",
    "for i, (query, ground_truth, predicted) in enumerate(zip(test_queries, ground_truth_answers, results)):\n",
    "    print(f\"\\nQuery {i+1}: {query}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(f\"SAS Score: {sas_evaluation['individual_scores'][i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
