{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DSPy with Amazon Bedrock\n",
    "---\n",
    "\n",
    "DSPy, developed by Stanford NLP, is an open-source library designed to streamline the process of creating and managing data science workflows. It is built around three core components: Signatures, Modules, and Optimizers.\n",
    "\n",
    "1. **Signatures**: Declarative specs of input/ output behavior of a module. This cleanly segregates what we want the module to do from how to do it. You provide some description of the fields (which will be used to build prompt) and field names carry semantic meaning as explained below.\n",
    "\n",
    "1. **Modules**: This is the core part of the program that manages the flow logic. DSPy provides built-in modules for basic Predict, Chain of Thought, ReAct, etc. You can create your own and compose multiple modules.\n",
    "\n",
    "1. **Optimizers**: The framework provides few optimizers(e.g. LabeledFewShot, BootstrapFewShotWithRandomSearc etc.) that tune prompt (adds examples based on random selection) and model parameters (e.g. temperature). They evaluate the performance based on the metric to optimize\n",
    "\n",
    "1. **Compiler**: Optimizes the instructions of Module and get relevant/ efficient examples for the task. The compiled program can be saved to disk and reloaded similar to checkpoints.\n",
    "\n",
    "_**Note**: DSPy aims to address the challenges of programming with language models by providing similar building blocks. It offers a composable pattern to represent individual units. DSPy doesnâ€™t eliminate language prompts altogether; instead, it builds the prompts based on the signatures, hints, and target models. DSPy helps crystallize our focus on writing the core logic along with signature field annotations and hints, rather than constructing lengthy prompts from scratch._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/stanfordnlp/dspy.git\n",
      "  Cloning https://github.com/stanfordnlp/dspy.git to /private/var/folders/jy/g9mb5j5n6c11fgdj788p5rww0000gr/T/pip-req-build-bncsf4f8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/stanfordnlp/dspy.git /private/var/folders/jy/g9mb5j5n6c11fgdj788p5rww0000gr/T/pip-req-build-bncsf4f8\n",
      "  Resolved https://github.com/stanfordnlp/dspy.git to commit f6b9d8b04a4046d0b996e6f344a619d3bc7e2eea\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: backoff~=2.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (1.4.2)\n",
      "Requirement already satisfied: openai in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (1.51.1)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (2.2.3)\n",
      "Requirement already satisfied: regex in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (2024.4.28)\n",
      "Requirement already satisfied: ujson in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (5.4.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (4.66.4)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.14.6 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (2.16.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (2.32.3)\n",
      "Requirement already satisfied: optuna in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (4.0.0)\n",
      "Requirement already satisfied: pydantic~=2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (2.9.2)\n",
      "Requirement already satisfied: structlog in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (24.4.0)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (3.1.3)\n",
      "Requirement already satisfied: magicattr~=0.1.6 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (0.1.6)\n",
      "Requirement already satisfied: litellm in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (1.49.0)\n",
      "Requirement already satisfied: diskcache in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from dspy-ai==2.5.8) (5.6.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (0.22.2)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (6.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai==2.5.8) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai==2.5.8) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pydantic~=2.0->dspy-ai==2.5.8) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->dspy-ai==2.5.8) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->dspy-ai==2.5.8) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->dspy-ai==2.5.8) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->dspy-ai==2.5.8) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jinja2->dspy-ai==2.5.8) (2.1.3)\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from litellm->dspy-ai==2.5.8) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from litellm->dspy-ai==2.5.8) (6.11.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from litellm->dspy-ai==2.5.8) (4.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from litellm->dspy-ai==2.5.8) (0.21.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from litellm->dspy-ai==2.5.8) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from litellm->dspy-ai==2.5.8) (0.15.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai->dspy-ai==2.5.8) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai->dspy-ai==2.5.8) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai->dspy-ai==2.5.8) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai->dspy-ai==2.5.8) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from openai->dspy-ai==2.5.8) (1.3.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from optuna->dspy-ai==2.5.8) (1.13.3)\n",
      "Requirement already satisfied: colorlog in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from optuna->dspy-ai==2.5.8) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from optuna->dspy-ai==2.5.8) (2.0.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas->dspy-ai==2.5.8) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas->dspy-ai==2.5.8) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas->dspy-ai==2.5.8) (2024.1)\n",
      "Requirement already satisfied: Mako in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from alembic>=1.5.0->optuna->dspy-ai==2.5.8) (1.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy-ai==2.5.8) (1.9.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->dspy-ai==2.5.8) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->dspy-ai==2.5.8) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm->dspy-ai==2.5.8) (3.17.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->dspy-ai==2.5.8) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->dspy-ai==2.5.8) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->dspy-ai==2.5.8) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->dspy-ai==2.5.8) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# To install DSPy run:\n",
    "# pip install dspy-ai\n",
    "\n",
    "# Install all of the latest packages within DSPy using the following command\n",
    "!pip install git+https://github.com/stanfordnlp/dspy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import dspy\n",
    "import logging\n",
    "\n",
    "# Import packages that are required to evaluate, load data, \n",
    "# compile the model with few shot examples and more\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple bedrock calls using DSPy\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-16 11:55:54,906] p31934 {credentials.py:1278} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "# Here, we initialize haiku on Amazon Bedrock by calling the AWSAnthropic on dspy. dspy has multiple different\n",
    "# options as well. A few include: AWSMeta, AWSMistral, AWSModel, etc.\n",
    "bedrock_haiku = dspy.AWSAnthropic(\n",
    "    aws_provider = dspy.Bedrock(region_name=\"us-west-2\"),\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    ")\n",
    "\n",
    "# configure dspy to use the bedrock_haiku model\n",
    "dspy.configure(lm=bedrock_haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predict(StringSignature(question -> answer\n",
       "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
       "))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple question and answer program\n",
    "qa = dspy.Predict(\"question -> answer\")\n",
    "\n",
    "# Predict is our module, its goal is to generate a prediction, and our signature is question -> answer\n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-16 11:56:13,008] p31934 {predict.py:16} WARNING - \t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated. ***\n",
      " \t\tYou are using the client AWSAnthropic, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Sergio Mattarella?\n",
      "Answer: Sergio Mattarella is the current President of Italy. He has held this position since 2015. Mattarella is a member of the Democratic Party and previously served as a member of the Italian Chamber of Deputies and the Constitutional Court of Italy.\n"
     ]
    }
   ],
   "source": [
    "simple_response: str = qa(question=\"Who is Sergio Mattarella?\").answer\n",
    "print(simple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy infers that question and answer are strings, and uses the prompt highlighted in instructions as input to the Language Model.\n",
    "# qa = dspy.TypedPredictor(\"question:str -> answer:int\")\n",
    "# qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DSPy in 8 steps\n",
    "---\n",
    "\n",
    "Using DSPy for solving a new tasks represents good machine learning with LLMs. It makes programming with LMs more easier, streamlined, and takes the heavy lifting of optimizing prompts from the scratch, LM weights, and other metrics. \n",
    "\n",
    "**The flow of using DSPy looks as such**: \n",
    "\n",
    "```\n",
    "Define your task\n",
    "    |_identify the metrics to be maximized\n",
    "        |_prepare a few shot examples/other compilation techniques\n",
    "                |_use built in modules (`ChainOfThought`, `ReACT`) with `signatures` (for input and output spec) defined \n",
    "                        |_Use a DSPy `optimizer` to compile the code into instructions, automatic few shot examples, updating LM weights \n",
    "                                |_evaluate the model responses\n",
    "```\n",
    "\n",
    "For an in depth walk through of these steps, view: [DSPy Documentation](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to modify the behavior of the program, we can tweak the signatures of the prompt and also the `module` based on the specific task\n",
    "\n",
    "#### DSPy module: Building block for programs that use LMs.\n",
    "\n",
    "Each built-in module abstracts a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any DSPy Signature.\n",
    "\n",
    "A DSPy module has learnable parameters (i.e., the little pieces comprising the prompt and the LM weights) and can be invoked (called) to process inputs and return outputs.\n",
    "\n",
    "#### Multiple modules can be composed into bigger modules (programs). DSPy modules are inspired directly by NN modules in PyTorch, but applied to LM programs.\n",
    "\n",
    "1. **dspy.Predict**: Basic predictor. Does not modify the signature. Handles the key forms of learning (i.e., storing the instructions and demonstrations and updates to the LM).\n",
    "\n",
    "1. **dspy.ChainOfThough**t: Teaches the LM to think step-by-step before committing to the signature's response.\n",
    "\n",
    "1. **dspy.ProgramOfThought**: Teaches the LM to output code, whose execution results will dictate the response.\n",
    "\n",
    "1. **dspy.ReAct**: An agent that can use tools to implement the given signature.\n",
    "\n",
    "1. **dspy.MultiChainComparison**: Can compare multiple outputs from ChainOfThought to produce a final prediction.\n",
    "\n",
    "We also have some function-style modules:\n",
    "\n",
    "**dspy.majority**: Can do basic voting to return the most popular response from a set of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Question: True of False: The numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\\nAnswer: True. The numbers 17, 9, 10, 12, 13, 4, and 2 add up to 67, which is an even number.'\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"True of False: The numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\"\n",
    "\n",
    "# Set signatures to the modules as to what type of input and output spec is required\n",
    "predictor = dspy.Predict(\"question -> answer\")\n",
    "predictor(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    rationale=\"Question: True of False: The numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\\nReasoning: Let's think step by step in order to determine if the numbers in this group add up to an even number.\\n1. We need to add up all the numbers in the group: 17 + 9 + 10 + 12 + 13 + 4 + 2 = 67.\\n2. 67 is an odd number, not an even number.\",\n",
       "    answer='False, the numbers in this group do not add up to an even number.'\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the ChainOfThought module to get step by step reasoning\n",
    "cot = dspy.ChainOfThought(\"question -> answer\")\n",
    "cot(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringSignature(question -> rationale, answer\n",
       "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    rationale = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${produce the answer}. We ...', '__dspy_field_type': 'output'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the details for the ChainOfThought Module signature, which contains an instruction, question\n",
    "# rationale and an answer\n",
    "cot.extended_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal Working Example\n",
    "In this post, we walk you through a minimal working example using the DSPy library.\n",
    "\n",
    "We make use of the GSM8K dataset and the Amazon Bedrock Claude 3 Haiku model to simulate prompting tasks within DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 32488.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 68626.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up the LM.\n",
    "bedrock_haiku = dspy.AWSAnthropic(\n",
    "    aws_provider = dspy.Bedrock(region_name=\"us-west-2\"),\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    ")\n",
    "dspy.settings.configure(lm=bedrock_haiku)\n",
    "\n",
    "# Load math questions from the GSM8K dataset.\n",
    "gsm8k = GSM8K()\n",
    "gsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Example({'question': \"The result from the 40-item Statistics exam Marion and Ella took already came out. Ella got 4 incorrect answers while Marion got 6 more than half the score of Ella. What is Marion's score?\", 'gold_reasoning': \"Ella's score is 40 items - 4 items = <<40-4=36>>36 items. Half of Ella's score is 36 items / 2 = <<36/2=18>>18 items. So, Marion's score is 18 items + 6 items = <<18+6=24>>24 items.\", 'answer': '24'}) (input_keys={'question'}), Example({'question': \"Stephen made 10 round trips up and down a 40,000 foot tall mountain. If he reached 3/4 of the mountain's height on each of his trips, calculate the total distance he covered.\", 'gold_reasoning': 'Up a mountain, Stephen covered 3/4*40000 = <<3/4*40000=30000>>30000 feet. Coming down, Stephen covered another 30000 feet, making the total distance covered in one round to be 30000+30000 = <<30000+30000=60000>>60000. Since Stephen made 10 round trips up and down the mountain, he covered 10*60000 = <<10*60000=600000>>600000', 'answer': '600000'}) (input_keys={'question'}), Example({'question': 'Bridget counted 14 shooting stars in the night sky.  Reginald counted two fewer shooting stars than did Bridget, but Sam counted four more shooting stars than did Reginald.  How many more shooting stars did Sam count in the night sky than was the average number of shooting stars observed for the three of them?', 'gold_reasoning': 'Reginald counted two fewer shooting stars than did Bridget, or a total of 14-2=<<14-2=12>>12 shooting stars. Sam counted 4 more shooting stars than did Reginald, or a total of 12+4=16 shooting stars. The average number of shooting stars observed for the three of them was (14+12+16)/3 = <<14=14>>14 shooting stars. Thus, Sam counted 16-14=2 more shooting stars than was the average number of shooting stars observed for the three of them.', 'answer': '2'}) (input_keys={'question'}), Example({'question': 'Sarah buys 20 pencils on Monday. Then she buys 18 more pencils on Tuesday. On Wednesday she buys triple the number of pencils she did on Tuesday. How many pencils does she have?', 'gold_reasoning': 'By adding together Monday and Tuesday, Saah has 20+18= <<20+18=38>>38 pencils On Wednesday, she buys 3 * 18= <<3*18=54>>54 pencils All together, Sarah has 38+54= <<38+54=92>>92 pencils', 'answer': '92'}) (input_keys={'question'}), Example({'question': 'Rookie police officers have to buy duty shoes at the full price of $85, but officers who have served at least a year get a 20% discount. Officers who have served at least three years get an additional 25% off the discounted price. How much does an officer who has served at least three years have to pay for shoes?', 'gold_reasoning': 'Cops that served a year pay $85 * 0.2 = $<<85*0.2=17>>17 less. Cops that served a year pay $85 - $17 = $<<85-17=68>>68. Cops that served at least 3 years get a $68 * 0.25 = $<<68*0.25=17>>17 discount. Cops that served at least 3 years pay $68 - $17 = $<<68-17=51>>51 for shoes.', 'answer': '51'}) (input_keys={'question'}), Example({'question': \"The average score on last week's Spanish test was 90.  Marco scored 10% less than the average test score and Margaret received 5 more points than Marco.  What score did Margaret receive on her test?\", 'gold_reasoning': 'The average test score was 90 and Marco scored 10% less so 90*.10 = <<90*.10=9>>9 points lower The average test score was 90 and Marco scored 9 points less so his test score was 90-9 = <<90-9=81>>81 Margret received 5 more points than Marco whose test score was 81 so she made 5+81 = <<5+81=86>>86 on her test', 'answer': '86'}) (input_keys={'question'}), Example({'question': 'A third of the contestants at a singing competition are female, and the rest are male. If there are 18 contestants in total, how many of them are male?', 'gold_reasoning': 'There are 18/3 = <<18/3=6>>6 female contestants. There are 18-6 = <<18-6=12>>12 male contestants.', 'answer': '12'}) (input_keys={'question'}), Example({'question': 'Nancy bought a pie sliced it into 8 pieces. She gave 1/2 to Joe and Darcy, and she gave 1/4 to Carl. How many slices were left?', 'gold_reasoning': 'The total number of slices she gave to Joe and Darcy is 1/2 x 8 = <<1/2*8=4>>4. The total slice she gave to Carl is 1/4 x 8 = <<1/4*8=2>>2. Therefore, the total slices left is 8 - 4 - 2 = <<8-4-2=2>>2.', 'answer': '2'}) (input_keys={'question'}), Example({'question': 'Megan pays $16 for a shirt that costs $22 before sales. What is the amount of the discount?', 'gold_reasoning': 'Let x be the amount of the discount. We have, 22 - x = $16 We change the writing of the equation: 22 - x + x = 16 + x So, 22 = 16 + x We then Remove 16 from both sides: 22 - 16 = 16 + x - 16 So, 22 - 16 = x So, the amount of the discount is x = $<<6=6>>6.', 'answer': '6'}) (input_keys={'question'}), Example({'question': \"Amaya scored 20 marks fewer in Maths than she scored in Arts. She also got 10 marks more in Social Studies than she got in Music. If she scored 70 in Music and scored 1/10 less in Maths, what's the total number of marks she scored in all the subjects?\", 'gold_reasoning': 'The total marks Amaya scored more in Music than in Maths is 1/10 * 70 = <<1/10*70=7>>7 marks. So the total marks she scored in Maths is 70 - 7 = <<70-7=63>>63 marks. If she scored 20 marks fewer in Maths than in Arts, then he scored 63 + 20 = <<63+20=83>>83 in Arts. If she scored 10 marks more in Social Studies than in Music, then she scored 70 + 10 = <<10+70=80>>80 marks in Social Studies. The total number of marks for all the subjects is 70 + 63 + 83 + 80 = <<70+63+83+80=296>>296 marks.', 'answer': '296'}) (input_keys={'question'})]\n"
     ]
    }
   ],
   "source": [
    "print(gsm8k_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Module\n",
    "\n",
    "With our environment set up, let's define a custom program that utilizes the ChainOfThought module to perform step-by-step reasoning to generate answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoT(dspy.Module):\n",
    "    \"\"\"\n",
    "    Chain of Thought (CoT) module for generating answers to questions.\n",
    "    This module takes a question as input and generates an answer as output.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(\"question -> answer\")\n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 4-shot examples of our CoT program.\n",
    "config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "\n",
    "# Optimize! Use the `gsm8k_metric` here. In general, the metric is going to tell the optimizer how well it's doing.\n",
    "teleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)\n",
    "optimized_cot = teleprompter.compile(CoT(), trainset=gsm8k_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "Now that we have a compiled (optimized) DSPy program, let's move to evaluating its performance on the dev dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 10  (90.0): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07<00:00,  1.29it/s]\n",
      "[2024-10-16 12:31:36,930] p31934 {evaluate.py:218} INFO - \u001b[2m2024-10-16T07:01:36.930192Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAverage Metric: 9 / 10 (90.0%)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m218\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the evaluator, which can be used multiple times.\n",
    "evaluate = Evaluate(devset=gsm8k_devset, metric=gsm8k_metric, num_threads=4, display_progress=True, display_table=0)\n",
    "\n",
    "# Evaluate our `optimized_cot` program.\n",
    "evaluate(optimized_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Human: Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Rookie police officers have to buy duty shoes at the full price of $85, but officers who have served at least a year get a 20% discount. Officers who have served at least three years get an additional 25% off the discounted price. How much does an officer who has served at least three years have to pay for shoes?\n",
      "Reasoning: Let's think step by step in order to Okay, let's think through this step-by-step. Question: Rookie police officers have to buy duty shoes at the full price of $85, but officers who have served at least a year get a 20% discount. Officers who have served at least three years get an additional 25% off the discounted price. How much does an officer who has served at least three years have to pay for shoes? Reasoning: Let's think step-by-step in order to calculate the final price. 1. The full price of the shoes is $85. 2. Officers who have served at least a year get a 20% discount. * 20% of $85 is $17 * $85 - $17 = $68 3. Officers who have served at least three years get an additional 25% off the discounted price. * 25% of $68 is $17 * $68 - $17 = $51 Therefore, an officer who has served at least three years has to pay $51 for the shoes.\n",
      "Answer: $51\n",
      "\n",
      "---\n",
      "\n",
      "Question: The average score on last week's Spanish test was 90. Marco scored 10% less than the average test score and Margaret received 5 more points than Marco. What score did Margaret receive on her test?\n",
      "Reasoning: Let's think step by step in order to Question: The average score on last week's Spanish test was 90. Marco scored 10% less than the average test score and Margaret received 5 more points than Marco. What score did Margaret receive on her test? Reasoning: Let's think step by step in order to produce the answer. We know that: - The average score on the Spanish test was 90. - Marco scored 10% less than the average test score. - Margaret received 5 more points than Marco. First, let's calculate Marco's score: - 10% less than 90 is 90 - (90 * 0.1) = 81. Now, let's calculate Margaret's score: - Margaret received 5 more points than Marco, so her score is 81 + 5 = 86.\n",
      "Answer: Margaret received a score of 86 on the Spanish test.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Nancy bought a pie sliced it into 8 pieces. She gave 1/2 to Joe and Darcy, and she gave 1/4 to Carl. How many slices were left?\n",
      "Reasoning: Let's think step by step in order to Question: Nancy bought a pie and sliced it into 8 pieces. She gave 1/2 to Joe and Darcy, and she gave 1/4 to Carl. How many slices were left? Reasoning: Let's think step by step in order to produce the answer. * The pie was sliced into 8 pieces * Nancy gave 1/2 of the pie to Joe and Darcy, which is 4 slices * Nancy gave 1/4 of the pie to Carl, which is 2 slices * So the total number of slices given away is 4 + 2 = 6 slices * The original pie had 8 slices, so the number of slices left is 8 - 6 = 2 slices\n",
      "Answer: 2 slices were left.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Amaya scored 20 marks fewer in Maths than she scored in Arts. She also got 10 marks more in Social Studies than she got in Music. If she scored 70 in Music and scored 1/10 less in Maths, what's the total number of marks she scored in all the subjects?\n",
      "Answer: 296\n",
      "\n",
      "---\n",
      "\n",
      "Question: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "Assistant:\u001b[32mOkay, let's think through this step-by-step.\n",
      "\n",
      "Question: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\n",
      "\n",
      "Reasoning: Let's think step-by-step in order to calculate the average number of bracelets Trey needs to sell each day.\n",
      "\n",
      "1. The bike costs $112.\n",
      "2. Trey plans to sell bracelets for $1 each over the next two weeks (14 days).\n",
      "3. To calculate the total number of bracelets he needs to sell, we divide the cost of the bike ($112) by the price of each bracelet ($1):\n",
      "   * $112 / $1 = 112 bracelets\n",
      "4. To find the average number of bracelets he needs to sell each day, we divide the total number of bracelets (112) by the number of days (14):\n",
      "   * 112 bracelets / 14 days = 8 bracelets per day\n",
      "\n",
      "Therefore, on average, Trey needs to sell 8 bracelets each day to raise enough money for the $112 bike.\n",
      "\n",
      "Answer: Trey needs to sell an average of 8 bracelets each day.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nHuman: Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rookie police officers have to buy duty shoes at the full price of $85, but officers who have served at least a year get a 20% discount. Officers who have served at least three years get an additional 25% off the discounted price. How much does an officer who has served at least three years have to pay for shoes?\\nReasoning: Let's think step by step in order to Okay, let's think through this step-by-step. Question: Rookie police officers have to buy duty shoes at the full price of $85, but officers who have served at least a year get a 20% discount. Officers who have served at least three years get an additional 25% off the discounted price. How much does an officer who has served at least three years have to pay for shoes? Reasoning: Let's think step-by-step in order to calculate the final price. 1. The full price of the shoes is $85. 2. Officers who have served at least a year get a 20% discount. * 20% of $85 is $17 * $85 - $17 = $68 3. Officers who have served at least three years get an additional 25% off the discounted price. * 25% of $68 is $17 * $68 - $17 = $51 Therefore, an officer who has served at least three years has to pay $51 for the shoes.\\nAnswer: $51\\n\\n---\\n\\nQuestion: The average score on last week's Spanish test was 90. Marco scored 10% less than the average test score and Margaret received 5 more points than Marco. What score did Margaret receive on her test?\\nReasoning: Let's think step by step in order to Question: The average score on last week's Spanish test was 90. Marco scored 10% less than the average test score and Margaret received 5 more points than Marco. What score did Margaret receive on her test? Reasoning: Let's think step by step in order to produce the answer. We know that: - The average score on the Spanish test was 90. - Marco scored 10% less than the average test score. - Margaret received 5 more points than Marco. First, let's calculate Marco's score: - 10% less than 90 is 90 - (90 * 0.1) = 81. Now, let's calculate Margaret's score: - Margaret received 5 more points than Marco, so her score is 81 + 5 = 86.\\nAnswer: Margaret received a score of 86 on the Spanish test.\\n\\n---\\n\\nQuestion: Nancy bought a pie sliced it into 8 pieces. She gave 1/2 to Joe and Darcy, and she gave 1/4 to Carl. How many slices were left?\\nReasoning: Let's think step by step in order to Question: Nancy bought a pie and sliced it into 8 pieces. She gave 1/2 to Joe and Darcy, and she gave 1/4 to Carl. How many slices were left? Reasoning: Let's think step by step in order to produce the answer. * The pie was sliced into 8 pieces * Nancy gave 1/2 of the pie to Joe and Darcy, which is 4 slices * Nancy gave 1/4 of the pie to Carl, which is 2 slices * So the total number of slices given away is 4 + 2 = 6 slices * The original pie had 8 slices, so the number of slices left is 8 - 6 = 2 slices\\nAnswer: 2 slices were left.\\n\\n---\\n\\nQuestion: Amaya scored 20 marks fewer in Maths than she scored in Arts. She also got 10 marks more in Social Studies than she got in Music. If she scored 70 in Music and scored 1/10 less in Maths, what's the total number of marks she scored in all the subjects?\\nAnswer: 296\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\nReasoning: Let's think step by step in order to\\n\\nAssistant:\\x1b[32mOkay, let's think through this step-by-step.\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step-by-step in order to calculate the average number of bracelets Trey needs to sell each day.\\n\\n1. The bike costs $112.\\n2. Trey plans to sell bracelets for $1 each over the next two weeks (14 days).\\n3. To calculate the total number of bracelets he needs to sell, we divide the cost of the bike ($112) by the price of each bracelet ($1):\\n   * $112 / $1 = 112 bracelets\\n4. To find the average number of bracelets he needs to sell each day, we divide the total number of bracelets (112) by the number of days (14):\\n   * 112 bracelets / 14 days = 8 bracelets per day\\n\\nTherefore, on average, Trey needs to sell 8 bracelets each day to raise enough money for the $112 bike.\\n\\nAnswer: Trey needs to sell an average of 8 bracelets each day.\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a deeper understanding of the model's interactions, we can review the most recent generations through \n",
    "# inspecting the model's history:\n",
    "bedrock_haiku.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate Example - Coding Assistant\n",
    "---\n",
    "\n",
    "Get LLM as a judge assessments on code samples generated using Claude 3 Sonnet using `llama3-1-70b instruct` as a judge. Generate code using custom chain of thought reasoning + coding dataset and generate improved code based on LLM as a judge feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Claude 3 Sonnet model on DSPy\n",
    "bedrock_claude_sonnet = dspy.AWSAnthropic(\n",
    "    aws_provider = dspy.Bedrock(region_name=\"us-west-2\"),\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    ")\n",
    "\n",
    "# Configure sonnet\n",
    "dspy.configure(lm=bedrock_claude_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    rationale=\"Question: Write a simple python function to calculate the first five items in the Fibonacci sequence.\\nReasoning: Let's think step by step in order to produce the code. We need to define a function that takes no arguments and returns a list containing the first five Fibonacci numbers. The Fibonacci sequence starts with 0 and 1, and each subsequent number is the sum of the two preceding ones. We can use a loop to calculate the next number in the sequence by summing the previous two numbers, and append it to a list until we have the desired number of elements.\",\n",
      "    code=\"```python\\ndef fibonacci():\\n    fib_list = [0, 1]\\n    for i in range(3, 6):\\n        next_num = fib_list[i-2] + fib_list[i-3]\\n        fib_list.append(next_num)\\n    return fib_list\\n```\\n\\nTo explain the code:\\n1. We define a function `fibonacci()`.\\n2. We initialize a list `fib_list` with the first two Fibonacci numbers, 0 and 1.\\n3. We use a `for` loop to iterate from 3 to 5 (range stops at 6, but doesn't include it).\\n4. Inside the loop, we calculate the next Fibonacci number by summing the previous two numbers in the list using their indices (`fib_list[i-2]` and `fib_list[i-3]`).\\n5. We append the new number to the `fib_list`.\\n6. After the loop finishes, we return the `fib_list` containing the first five Fibonacci numbers.\\n\\nTo use the function, you can call it like this:\\n\\n```python\\nresult = fibonacci()\\nprint(result)  # Output: [0, 1, 1, 2, 3]\\n```\"\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Simply invoke claude 3 sonnet \n",
    "qa = dspy.ChainOfThought('question -> code')\n",
    "question = \"Write a simple python function to calculate the first five items in the Fibonacci sequence.\"\n",
    "response = qa(question=question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class code_gen(dspy.Signature):\n",
    "    \"\"\"Generate simple python functions to solve for user questions using best coding practices, such as \n",
    "    typing hints, comments, and minimal code that gives the correct response\"\"\"\n",
    "    \n",
    "    question = dspy.InputField(desc='This is the question about a coding related problem. For all other un related questions, say \"My task is to provide code to programming questions, I cannot help with this question\"')\n",
    "    code = dspy.OutputField(desc='This contains the simple python code to solve the question provided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a simple python function to calculate the first five items in the Fibonacci sequence.\"\n",
    "\n",
    "code_gen = dspy.Predict(code_gen)\n",
    "response = code_gen(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    code='Question: Write a simple python function to calculate the first five items in the Fibonacci sequence.\\nCode:\\n\\n```python\\ndef fibonacci(n: int) -> list[int]:\\n    \"\"\"\\n    Returns the first n Fibonacci numbers as a list.\\n    \\n    Args:\\n        n (int): The number of Fibonacci numbers to generate.\\n        \\n    Returns:\\n        list[int]: A list containing the first n Fibonacci numbers.\\n    \"\"\"\\n    fib_list = []\\n    a, b = 0, 1\\n    for _ in range(n):\\n        fib_list.append(a)\\n        a, b = b, a + b\\n    return fib_list\\n\\n# Example usage\\nprint(fibonacci(5))  # Output: [0, 1, 1, 2, 3]\\n```\\n\\nExplanation:\\n- The `fibonacci` function takes an integer `n` as input and returns a list of the first `n` Fibonacci numbers.\\n- The function uses type hints (`int` and `list[int]`) to specify the expected input and output types.\\n- The docstring explains the purpose of the function, its arguments, and its return value.\\n- The function initializes an empty list `fib_list` to store the Fibonacci numbers.\\n- It also initializes the first two Fibonacci numbers `a` and `b` as 0 and 1, respectively.\\n- The function then iterates `n` times using a `for` loop.\\n- In each iteration, it appends the current value of `a` to `fib_list`, and updates `a` and `b` to the next Fibonacci numbers.\\n- Finally, the function returns the `fib_list` containing the first `n` Fibonacci numbers.\\n- The example usage demonstrates how to call the `fibonacci` function with `n=5` and prints the resulting list `[0, 1, 1, 2, 3]`.'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For un related questions\n",
    "question = \"What is the capital of India?\"\n",
    "\n",
    "code_gen = dspy.Predict(code_gen)\n",
    "response = code_gen(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    code='My task is to provide code to programming questions, I cannot help with this question.'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a `coding-prompts-small` Dataset from hugging face using the DSPy `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader()\n",
    "\n",
    "perl_code_ds = dl.from_huggingface(\n",
    "    \"perlthoughts/coding-prompts-small\",\n",
    "    input_keys=(\"instruction\",)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssessCode(dspy.Signature):\n",
    "    \"\"\"Assess the quality and correctness of the python code compared to the ground truth.\"\"\"\n",
    "\n",
    "    generated_code = dspy.InputField(desc=\"Generated Python code to be assessed\")\n",
    "    ground_truth_code = dspy.InputField(desc=\"Ground truth Python code\")\n",
    "    task_description = dspy.InputField(desc=\"Description of the coding task\")\n",
    "    assessment = dspy.OutputField(desc=\"Detailed assessment of the generated code's correctness and quality\")\n",
    "\n",
    "# This is the llm that acts as a judge to evaluate responses from claude 3 sonnet\n",
    "bedrock_llama3_1_70b_judge = dspy.AWSMeta(\n",
    "    aws_provider = dspy.Bedrock(region_name=\"us-west-2\"),\n",
    "    model=\"meta.llama3-1-70b-instruct-v1:0\",\n",
    ")\n",
    "\n",
    "# Configure DSPy\n",
    "dspy.configure(lm=bedrock_llama3_1_70b_judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a feedback metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_correctness_metric(dataset_item, pred, trace=None):\n",
    "    task_description = dataset_item.task_description\n",
    "    ground_truth_code = dataset_item.ground_truth_code\n",
    "    generated_code = pred.output\n",
    "\n",
    "    assessment_prompt = f\"\"\"\n",
    "    Task Description: {task_description}\n",
    "    \n",
    "    Ground Truth Code:\n",
    "    {ground_truth_code}\n",
    "    \n",
    "    Generated Code:\n",
    "    {generated_code}\n",
    "    \n",
    "    Please assess the generated code based on the following criteria:\n",
    "    1. Correctness: Does the generated code produce the same output as the ground truth code?\n",
    "    2. Efficiency: Is the generated code as efficient as or more efficient than the ground truth code?\n",
    "    3. Readability: Is the generated code clean and easy to understand?\n",
    "    4. Completeness: Does the generated code fully address all aspects of the task description?\n",
    "    \n",
    "    Provide a detailed assessment and a score from 0 to 1, where 1 is perfect and 0 is completely incorrect.\n",
    "    \"\"\"\n",
    "\n",
    "    with dspy.context(lm=bedrock_llama3_1_70b_judge):\n",
    "        assessment = dspy.Predict(AssessCode)(\n",
    "            generated_code=generated_code,\n",
    "            ground_truth_code=ground_truth_code,\n",
    "            task_description=assessment_prompt\n",
    "        )\n",
    "\n",
    "    # Extract the score from the assessment (assuming the LLM includes a score in its output)\n",
    "    # You might need to adjust this part based on the actual output format of your LLM\n",
    "    score_line = [line for line in assessment.assessment.split('\\n') if 'Score:' in line]\n",
    "    if score_line:\n",
    "        score = float(score_line[0].split(':')[1].strip())\n",
    "    else:\n",
    "        score = 0  # Default to 0 if no score is found\n",
    "\n",
    "    if trace is not None:\n",
    "        return score >= 0.8  # You can adjust this threshold as needed\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_3_generator = dspy.AWSAnthropic(\n",
    "    aws_provider = dspy.Bedrock(region_name=\"us-west-2\"),\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    ")\n",
    "\n",
    "dspy.configure(lm=bedrock_haiku)\n",
    "\n",
    "# Define a signature for code generation\n",
    "class GenerateCode(dspy.Signature):\n",
    "    instruction = dspy.InputField(desc=\"Instruction on what kind of python code to generate\")\n",
    "    code = dspy.OutputField(desc=\"Generated python code\")\n",
    "\n",
    "# Create a DSPy module for code generation\n",
    "class CodeGenerator(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(GenerateCode)\n",
    "\n",
    "    def forward(self, instruction):\n",
    "        return self.generate(instruction=instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CodeGenerator\n",
    "code_generator = CodeGenerator()\n",
    "\n",
    "# Function to process each example\n",
    "# Function to process each example\n",
    "def process_example(example):\n",
    "    # Generate code using Claude 3\n",
    "    with dspy.context(lm=claude_3_generator):\n",
    "        generated = code_generator(example.instruction)\n",
    "    \n",
    "    # Evaluate the generated code using Llama 3\n",
    "    with dspy.context(lm=bedrock_llama3_1_70b_judge):\n",
    "        assessment = dspy.Predict(AssessCode)(\n",
    "            generated_code=generated.code,\n",
    "            ground_truth_code=example.output,\n",
    "            task_description=example.instruction\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"instruction\": example.instruction,\n",
    "        \"generated_code\": generated.code,\n",
    "        \"ground_truth\": example.output,\n",
    "        \"assessment\": assessment.assessment\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-16 12:39:03,702] p31934 {predict.py:16} WARNING - \t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated. ***\n",
      " \t\tYou are using the client AWSMeta, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed example. Assessment:\n",
      "**Assessment**\n",
      "\n",
      "**Correctness: 8/10**\n",
      "\n",
      "The generated code is mostly correct, but it does not exactly match the ground truth code. The generated code creates an empty list `even_numbers` and then appends even numbers from 1 to 10 to it using a loop. However, the ground truth code is a simple list literal containing the even numbers between 1 and 10.\n",
      "\n",
      "The generated code produces the correct output, but it is more verbose and less efficient than the ground truth code.\n",
      "\n",
      "**Quality: 6/10**\n",
      "\n",
      "The generated code has some quality issues:\n",
      "\n",
      "* The variable name `even_numbers` is not very descriptive. A more descriptive name, such as `even_numbers_between_1_and_10`, would be better.\n",
      "* The code uses a loop to generate the list of even numbers, which is unnecessary when a simple list literal can be used.\n",
      "* The code does not follow the principle of \"Don't Repeat Yourself\" (DRY), as the range of numbers is hardcoded in the `range` function.\n",
      "\n",
      "**Suggestions for Improvement**\n",
      "\n",
      "* Use a more descriptive variable name.\n",
      "* Use a list literal instead of a loop to generate the list of even numbers.\n",
      "* Consider using a more concise and efficient way to generate the list of even numbers, such as using a list comprehension.\n",
      "\n",
      "**Example of Improved Code**\n",
      "\n",
      "`even_numbers_between_1_and_10 = [i for i in range(1, 11) if i % 2 == 0]`\n",
      "\n",
      "This improved code uses a list comprehension to generate the list of even numbers between 1 and 10, which is more concise and efficient than the original generated code.\n",
      "\n",
      "Processed example. Assessment:\n",
      "Assessment:\n",
      "\n",
      "**Correctness: 6/10**\n",
      "\n",
      "The generated code is partially correct. It correctly calculates the sine of the given angle and uses it to calculate the height of the triangle. However, it does not take into account the side length in the calculation, which is a crucial component of the formula. The ground truth code clearly states that the height of the triangle is equal to the opposite side length multiplied by the sine of the angle, divided by the side length.\n",
      "\n",
      "**Quality: 8/10**\n",
      "\n",
      "The generated code is well-structured and readable. It includes a clear docstring that explains the purpose of the function, its arguments, and its return value. The variable names are descriptive, and the code is concise. However, the code does not handle potential errors, such as invalid input values or division by zero.\n",
      "\n",
      "**Suggestions for Improvement:**\n",
      "\n",
      "1. Correct the formula: Update the code to use the correct formula, which includes dividing by the side length.\n",
      "2. Add error handling: Include checks to ensure that the input values are valid and handle potential errors, such as division by zero.\n",
      "3. Simplify the code: The code can be simplified by removing the unnecessary `sin_angle` variable and directly using the `math.sin(angle)` value in the calculation.\n",
      "\n",
      "**Corrected Code:**\n",
      "```python\n",
      "import math\n",
      "\n",
      "def calculate_triangle_height(angle, side_a, side_b, opposite_side):\n",
      "    \"\"\"\n",
      "    Calculate the height of a triangle given the angle, side lengths, and opposite side length.\n",
      "\n",
      "    Args:\n",
      "        angle (float): The angle in radians between the two sides (side_a and side_b).\n",
      "        side_a (float): The length of one side of the triangle.\n",
      "        side_b (float): The length of the other side of the triangle.\n",
      "        opposite_side (float): The length of the side opposite to the given angle.\n",
      "\n",
      "    Returns:\n",
      "        float: The height of the triangle.\n",
      "    \"\"\"\n",
      "    if side_a <= 0 or side_b <= 0 or opposite_side <= 0:\n",
      "        raise ValueError(\"Side lengths must be positive\")\n",
      "\n",
      "    height = opposite_side * math.sin(angle) / side_a\n",
      "    return height\n",
      "\n",
      "# Example usage\n",
      "angle_radians = math.pi / 3  # 60 degrees in radians\n",
      "side_a = 4\n",
      "side_b = 5\n",
      "opposite_side = 6\n",
      "height = calculate_triangle_height(angle_radians, side_a, side_b, opposite_side)\n",
      "print(f\"The height of the triangle is: {height:.2f}\")\n",
      "```\n",
      "\n",
      "Processed example. Assessment:\n",
      "Assessment:\n",
      "\n",
      "**Correctness:**\n",
      "\n",
      "The generated code is partially correct. It correctly implements a `replace` method that replaces a given set of characters with a new string. However, it has a few issues:\n",
      "\n",
      "* The `replace` method does not handle the case where `old` is a set of characters, as specified in the task description. Instead, it checks if `char` is present in `old` using the `in` operator, which will only work correctly if `old` is a single character.\n",
      "* The `replace` method does not handle the case where `new` is an empty string. In this case, the method will append an empty string to `new_string` for each occurrence of `old`, resulting in an incorrect output.\n",
      "\n",
      "The ground truth code, on the other hand, is incorrect. It only replaces spaces with the given string, and does not handle the general case of replacing a given set of characters.\n",
      "\n",
      "**Quality:**\n",
      "\n",
      "The generated code has some quality issues:\n",
      "\n",
      "* The `replace` method is not efficient, as it creates a new string by concatenating characters one by one. This can be slow for large strings. A more efficient approach would be to use a list to store the characters and then join them into a string at the end.\n",
      "* The `replace` method does not handle edge cases, such as an empty input string or a null input string.\n",
      "* The code does not follow PEP 8 conventions, such as using consistent indentation and spacing.\n",
      "\n",
      "The ground truth code is also of poor quality, as it is incomplete and does not handle the general case of replacing a given set of characters.\n",
      "\n",
      "**Suggestions for improvement:**\n",
      "\n",
      "* Modify the `replace` method to handle sets of characters correctly.\n",
      "* Add error handling for edge cases, such as empty or null input strings.\n",
      "* Use a more efficient approach to building the new string, such as using a list and joining it at the end.\n",
      "* Follow PEP 8 conventions for coding style and formatting.\n",
      "\n",
      "Here is an example of how the improved code could look:\n",
      "```python\n",
      "class MyString:\n",
      "    def __init__(self, string):\n",
      "        self.string = string\n",
      "\n",
      "    def replace(self, old, new):\n",
      "        if not isinstance(old, str):\n",
      "            raise ValueError(\"old must be a string\")\n",
      "        if not isinstance(new, str):\n",
      "            raise ValueError(\"new must be a string\")\n",
      "        if not self.string:\n",
      "            return \"\"\n",
      "\n",
      "        new_string = []\n",
      "        for char in self.string:\n",
      "            if char in old:\n",
      "                new_string.append(new)\n",
      "            else:\n",
      "                new_string.append(char)\n",
      "        return \"\".join(new_string)\n",
      "```\n",
      "This improved code handles sets of characters correctly, adds error handling for edge cases, and uses a more efficient approach to building the new string, and follows PEP 8 conventions for coding style and formatting.\n",
      "\n",
      "Processed example. Assessment:\n",
      "Assessment:\n",
      "\n",
      "**Correctness:**\n",
      "\n",
      "The generated code is mostly correct, but it has a logical flaw. The `break` statement is unnecessary and can lead to incorrect results if the range or step size is changed. The `range` function will automatically stop generating numbers when it reaches the end of the specified range, so the `break` statement is not needed.\n",
      "\n",
      "Additionally, the generated code uses a conditional statement to check the length of the list, which is not necessary. The `range` function can be used to generate the exact number of elements needed.\n",
      "\n",
      "**Quality:**\n",
      "\n",
      "The generated code has some quality issues:\n",
      "\n",
      "* The variable name `numbers` is not very descriptive. A more descriptive name, such as `multiples_of_three`, would be better.\n",
      "* The code uses a loop to append elements to a list, which is not the most efficient way to create a list in Python. A list comprehension or the `range` function with slicing would be more efficient and readable.\n",
      "* The code uses a magic number (15) to determine the length of the list. It would be better to define a constant or variable for this value to make the code more readable and maintainable.\n",
      "\n",
      "**Comparison to Ground Truth:**\n",
      "\n",
      "The ground truth code is a simple and efficient list literal that directly creates the desired array. The generated code, on the other hand, uses a loop and conditional statements to create the array, which is less efficient and less readable.\n",
      "\n",
      "**Improved Version:**\n",
      "\n",
      "Here is an improved version of the generated code that addresses the correctness and quality issues:\n",
      "```python\n",
      "multiples_of_three = list(range(3, 46, 3))[:15]\n",
      "print(multiples_of_three)\n",
      "```\n",
      "This code uses a list comprehension to create the array, which is more efficient and readable. It also uses slicing to limit the length of the array to 15 elements, which is more concise and readable than using a conditional statement.\n",
      "\n",
      "Processed example. Assessment:\n",
      "Assessment:\n",
      "\n",
      "**Correctness:**\n",
      "\n",
      "The generated code is mostly correct, but it has a subtle issue. The `distinct_states` set is storing tuples of rows, which is not the intended behavior. The ground truth code correctly converts each row to a string before adding it to the set. This is because sets in Python cannot contain mutable objects like lists, and tuples are not suitable for this task because they are ordered and would treat `[1, 0, 0, 1]` and `[1, 0, 0, 1]` as the same state even if they appear in different rows.\n",
      "\n",
      "The generated code would produce incorrect results if the input matrix contains duplicate rows with different orders of elements.\n",
      "\n",
      "**Quality:**\n",
      "\n",
      "The generated code is concise and readable, but it could be improved in a few ways:\n",
      "\n",
      "* The variable name `distinct_states` could be more descriptive, such as `unique_states` or `distinct_rows`.\n",
      "* The function name `count_distinct_states` could be more descriptive, such as `find_num_distinct_states` (as in the ground truth code).\n",
      "* The code could benefit from a docstring to explain its purpose and behavior.\n",
      "* The example usage could be included in a separate section or as a test case, rather than being part of the code itself.\n",
      "\n",
      "**Comparison to Ground Truth Code:**\n",
      "\n",
      "The ground truth code is more correct and idiomatic Python. It uses a more descriptive function name and variable names, and it correctly converts each row to a string before adding it to the set. The generated code could be improved by adopting these best practices.\n",
      "\n",
      "**Suggestions for Improvement:**\n",
      "\n",
      "* Convert each row to a string before adding it to the set, as in the ground truth code.\n",
      "* Use more descriptive variable names and function names.\n",
      "* Add a docstring to explain the purpose and behavior of the function.\n",
      "* Consider including example usage as a separate test case or in a separate section.\n",
      "\n",
      "Processed example. Assessment:\n",
      "**Assessment**\n",
      "\n",
      "**Correctness: 8/10**\n",
      "\n",
      "The generated code is mostly correct, but it has a minor issue. The `end=\" \"` argument in the `print` function is not necessary and can be removed. The `print` function already appends a space after each number, so the `end` argument is redundant. Additionally, the generated code uses f-strings to concatenate the numbers, which is not necessary in this case. The ground truth code simply uses the `print` function with multiple arguments, which is more straightforward.\n",
      "\n",
      "**Quality: 7/10**\n",
      "\n",
      "The generated code is readable, but it can be improved. The use of f-strings and the `end` argument makes the code slightly more complex than necessary. The ground truth code is more concise and easier to understand. Additionally, the generated code does not follow the PEP 8 style guide, which recommends using a space after the comma in the `print` function.\n",
      "\n",
      "**Suggestions for Improvement**\n",
      "\n",
      "* Remove the `end` argument from the `print` function.\n",
      "* Use the `print` function with multiple arguments instead of f-strings.\n",
      "* Follow the PEP 8 style guide for spacing and formatting.\n",
      "\n",
      "**Corrected Generated Code**\n",
      "\n",
      "```python\n",
      "for i in range(10):\n",
      "    for j in range(10):\n",
      "        print(i, j)\n",
      "```\n",
      "\n",
      "This corrected code is identical to the ground truth code and is more concise and readable.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m perl_code_ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]:  \u001b[38;5;66;03m# Assuming 'train' is the key for the dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         result \u001b[38;5;241m=\u001b[39m process_example(example)\n\u001b[1;32m      6\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed example. Assessment:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massessment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 13\u001b[0m, in \u001b[0;36mprocess_example\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate the generated code using Llama 3\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mbedrock_llama3_1_70b_judge):\n\u001b[0;32m---> 13\u001b[0m     assessment \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(AssessCode)(\n\u001b[1;32m     14\u001b[0m         generated_code\u001b[38;5;241m=\u001b[39mgenerated\u001b[38;5;241m.\u001b[39mcode,\n\u001b[1;32m     15\u001b[0m         ground_truth_code\u001b[38;5;241m=\u001b[39mexample\u001b[38;5;241m.\u001b[39moutput,\n\u001b[1;32m     16\u001b[0m         task_description\u001b[38;5;241m=\u001b[39mexample\u001b[38;5;241m.\u001b[39minstruction\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: example\u001b[38;5;241m.\u001b[39minstruction,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_code\u001b[39m\u001b[38;5;124m\"\u001b[39m: generated\u001b[38;5;241m.\u001b[39mcode,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m: example\u001b[38;5;241m.\u001b[39moutput,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massessment\u001b[39m\u001b[38;5;124m\"\u001b[39m: assessment\u001b[38;5;241m.\u001b[39massessment\n\u001b[1;32m     24\u001b[0m }\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dspy/predict/predict.py:119\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dspy/predict/predict.py:164\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         completions \u001b[38;5;241m=\u001b[39m new_generate(lm, signature, dsp\u001b[38;5;241m.\u001b[39mExample(demos\u001b[38;5;241m=\u001b[39mdemos, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m         completions \u001b[38;5;241m=\u001b[39m old_generate(demos, signature, kwargs, config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage)\n\u001b[1;32m    166\u001b[0m pred \u001b[38;5;241m=\u001b[39m Prediction\u001b[38;5;241m.\u001b[39mfrom_completions(completions, signature\u001b[38;5;241m=\u001b[39msignature)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mtrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dspy/predict/predict.py:191\u001b[0m, in \u001b[0;36mold_generate\u001b[0;34m(demos, signature, kwargs, config, lm, stage)\u001b[0m\n\u001b[1;32m    188\u001b[0m template \u001b[38;5;241m=\u001b[39m signature_to_template(signature)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m dsp\u001b[38;5;241m.\u001b[39mgenerate(template, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)(x, stage\u001b[38;5;241m=\u001b[39mstage)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# Note: query_only=True means the instructions and examples are not included.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dsp/primitives/predict.py:73\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[0;32m---> 73\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m generator(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     74\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dsp/modules/aws_models.py:128\u001b[0m, in \u001b[0;36mAWSModel.__call__\u001b[0;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_sorted:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_sorted must be False for now\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_request(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [generated]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dsp/modules/aws_models.py:109\u001b[0m, in \u001b[0;36mAWSModel.basic_request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError - input tokens \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeds max context \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_context_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    108\u001b[0m formatted_prompt: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_prompt(prompt)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simple_api_call(formatted_prompt\u001b[38;5;241m=\u001b[39mformatted_prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dsp/modules/aws_models.py:91\u001b[0m, in \u001b[0;36mAWSModel._simple_api_call\u001b[0;34m(self, formatted_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     llm_out \u001b[38;5;241m=\u001b[39m [generated\u001b[38;5;241m.\u001b[39mreplace(formatted_prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m generated \u001b[38;5;129;01min\u001b[39;00m llm_out]\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     llm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_model(json_body)\n\u001b[1;32m     92\u001b[0m     llm_out \u001b[38;5;241m=\u001b[39m llm_out\u001b[38;5;241m.\u001b[39mreplace(formatted_prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     95\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: formatted_prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: llm_out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: body},\n\u001b[1;32m     96\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dsp/modules/aws_models.py:322\u001b[0m, in \u001b[0;36mAWSMeta._call_model\u001b[0;34m(self, body)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, body: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maws_provider\u001b[38;5;241m.\u001b[39mcall_model(\n\u001b[1;32m    323\u001b[0m         model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name,\n\u001b[1;32m    324\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maws_provider, Bedrock):\n\u001b[1;32m    327\u001b[0m         response_body \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dsp/modules/aws_providers.py:132\u001b[0m, in \u001b[0;36mBedrock.call_model\u001b[0;34m(self, model_id, body)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_id: \u001b[38;5;28mstr\u001b[39m, body: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39minvoke_model(\n\u001b[1;32m    133\u001b[0m         modelId\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    134\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    135\u001b[0m         accept\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    136\u001b[0m         contentType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    137\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/client.py:1005\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     maybe_compress_request(\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[1;32m   1003\u001b[0m     )\n\u001b[1;32m   1004\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m-> 1005\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m   1006\u001b[0m         operation_model, request_dict, request_context\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1011\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1015\u001b[0m )\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/client.py:1029\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1029\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpoint\u001b[38;5;241m.\u001b[39mmake_request(operation_model, request_dict)\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1032\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1033\u001b[0m             exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   1034\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1035\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(request_dict, operation_model)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/endpoint.py:197\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m    196\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[0;32m--> 197\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[1;32m    198\u001b[0m     request, operation_model, context\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    201\u001b[0m     attempts,\n\u001b[1;32m    202\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     exception,\n\u001b[1;32m    206\u001b[0m ):\n\u001b[1;32m    207\u001b[0m     attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/endpoint.py:239\u001b[0m, in \u001b[0;36mEndpoint._get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, operation_model, context):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# This will return a tuple of (success_response, exception)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# and success_response is itself a tuple of\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# (http_response, parsed_dict).\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# If an exception occurs then the success_response is None.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# If no exception occurs then exception is None.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_get_response(\n\u001b[1;32m    240\u001b[0m         request, operation_model, context\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    242\u001b[0m     kwargs_to_emit \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_response\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: context,\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m'\u001b[39m: exception,\n\u001b[1;32m    247\u001b[0m     }\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/endpoint.py:279\u001b[0m, in \u001b[0;36mEndpoint._do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    277\u001b[0m     http_response \u001b[38;5;241m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m http_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         http_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(request)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, e)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/endpoint.py:383\u001b[0m, in \u001b[0;36mEndpoint._send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_session\u001b[38;5;241m.\u001b[39msend(request)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/botocore/httpsession.py:464\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    461\u001b[0m     conn\u001b[38;5;241m.\u001b[39mproxy_headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m host\n\u001b[1;32m    463\u001b[0m request_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_target(request\u001b[38;5;241m.\u001b[39murl, proxy_url)\n\u001b[0;32m--> 464\u001b[0m urllib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    465\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    466\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest_target,\n\u001b[1;32m    467\u001b[0m     body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    468\u001b[0m     headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    469\u001b[0m     retries\u001b[38;5;241m=\u001b[39mRetry(\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    470\u001b[0m     assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    471\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    472\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    473\u001b[0m     chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunked(request\u001b[38;5;241m.\u001b[39mheaders),\n\u001b[1;32m    474\u001b[0m )\n\u001b[1;32m    476\u001b[0m http_response \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mawsrequest\u001b[38;5;241m.\u001b[39mAWSResponse(\n\u001b[1;32m    477\u001b[0m     request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    478\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    479\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    480\u001b[0m     urllib_response,\n\u001b[1;32m    481\u001b[0m )\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request\u001b[38;5;241m.\u001b[39mstream_output:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# Cause the raw stream to be exhausted immediately. We do it\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# this way instead of using preload_content because\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# preload_content will never buffer chunked responses\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    717\u001b[0m     conn,\n\u001b[1;32m    718\u001b[0m     method,\n\u001b[1;32m    719\u001b[0m     url,\n\u001b[1;32m    720\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    721\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    722\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    723\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    724\u001b[0m )\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:463\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/http/client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1386\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "results = []\n",
    "for example in perl_code_ds['train']:  # Assuming 'train' is the key for the dataset\n",
    "    try:\n",
    "        result = process_example(example)\n",
    "        results.append(result)\n",
    "        print(f\"Processed example. Assessment:\\n{result['assessment']}\\n\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"Skipping example due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Print all results\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Instruction: {result['instruction']}\")\n",
    "    print(f\"Generated Code:\\n{result['generated_code']}\")\n",
    "    print(f\"Ground Truth:\\n{result['ground_truth']}\")\n",
    "    print(f\"Assessment:\\n{result['assessment']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
