{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt flow RAG example\n",
    "---\n",
    "\n",
    "Amazon Bedrock Flows accelerates the creation, testing, and deployment of user-defined workflows for generative AI applications through an intuitive visual builder. Using Bedrock prompt flows, users can drag, drop and link Prompts, existing Agents, Knowledge Bases, Guardrails and other AWS services. This enables generative AI teams to build a business logic via workflow creations. \n",
    "\n",
    "In this example, we will be building a simple RAG application. We will follow the following steps:\n",
    "\n",
    "1. We will be creating a RAG prompt. This prompt will be stored in Amazon Bedrock within the prompt management feature functionality. Prompt management on Bedrock simplifies the creation, evaluation, versioning, and sharing of prompts to help developers and prompt engineers get the best responses from foundation models (FMs) for their use cases. \n",
    "\n",
    "1. Next, we will create a knowledge base, which will store sample information about AWS services. \n",
    "\n",
    "1. We will create a prompt flow with the user provided prompt, retrieval of chunks from the knowledge base via a lambda function, followed by answering the user defined question. We will also introduce Guardrails and update our flow at the end.\n",
    "\n",
    "1. Lastly, we want to be able to access the prompt flow from an external application via an API or export it via a CloudFormation template. We will be doing so at the end. This helps developers to seamlessly access their RAG applications through a simple invocation.\n",
    "\n",
    "\n",
    "[Amazon Bedrock Prompt Management](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management.html) streamlines the creation, evaluation, deployment, and sharing of prompts in the Amazon Bedrock console and via APIs in the SDK. This feature helps developers and business users obtain the best responses from foundation models for their specific use cases.\n",
    "\n",
    "[Amazon Bedrock Prompt Flows](https://docs.aws.amazon.com/bedrock/latest/userguide/flows.html) allows you to easily link multiple foundation models (FMs), prompts, and other AWS services, reducing development time and effort. It introduces a visual builder in the Amazon Bedrock console and a new set of APIs in the SDK, that simplifies the creation of complex generative AI workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making sure we have the lastest version of the Amazon Bedrock SDK, importing the libraries, and setting-up the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this the first time...\n",
    "!pip3 install boto3 botocore matplotlib -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# This is the model that will be used for standard text generation for our\n",
    "# RAG application use case\n",
    "TEXT_GEN_MODEL: str = \"amazon.nova-lite-v1:0\"\n",
    "# S3 bucket that contains information on the PDF file that is stored within the Knowledge base\n",
    "S3_INPUT_BUCKET: str = \"genai-appdev-frameworks\"\n",
    "# This is the link of the PDF file that is stored within the Knowledge base\n",
    "URL: str = 'https://d0.awsstatic.com/whitepapers/aws-overview.pdf'\n",
    "# Local directory where the data is saved\n",
    "DATA_DIR: str = \"data\"\n",
    "# PDF file name to be saved and put in S3\n",
    "PDF_FILE_NAME: str = \"aws-overview.pdf\"\n",
    "# This is the name of the Knowledge base that will be created\n",
    "KB_NAME: str = \"aws-overview-kb\"\n",
    "KB_DESC: str = \"\"\"Use this KB to retrieve information about AWS services that the user is asking about.\"\"\"\n",
    "\n",
    "# This is the lambda function that will be used to fetch the top 5 chunks from the knowledge base\n",
    "KB_LAMBDA_FUNCTION_NAME: str = 'demo-lambda-function'\n",
    "KB_LAMBDA_FUNCTION: str = 'kb-lambda-function.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the file locally, put it in S3, and then sync it with the Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "try:\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "    response = requests.get(URL)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Save the file\n",
    "    file_path = os.path.join(DATA_DIR, PDF_FILE_NAME)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    logger.info(f\"File successfully downloaded to {file_path}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logger.info(f\"Error downloading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "# Next, put the data contents in the input s3 bucket. We will use this s3 bucket content to sync with the knowledge base after \n",
    "# creating it\n",
    "try:\n",
    "    file_path = os.path.join(DATA_DIR, PDF_FILE_NAME)\n",
    "    s3_client.upload_file(\n",
    "        file_path,           \n",
    "        S3_INPUT_BUCKET,     \n",
    "        PDF_FILE_NAME \n",
    "    )\n",
    "    logger.info(f\"Successfully uploaded {PDF_FILE_NAME} to {S3_INPUT_BUCKET}\")\n",
    "except Exception as e:\n",
    "    logger.info(f\"Error uploading to S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the current AWS region\n",
    "region = boto3.client('sts').meta.region_name # change to another region if you do not want\n",
    "# the region to be dynamically fetched\n",
    "logger.info(f\"Current AWS region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent = boto3.client(service_name = \"bedrock-agent\", region_name = region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Application Prompt\n",
    "\n",
    "Let's create our sample RAG application prompt by leveraging on Prompt Management for Amazon Bedrock. Here, you can adjust the sample prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt(\n",
    "    name = f\"aws-expert-assistant\",\n",
    "    description = \"Prompt template for AWS expert assistant to analyze content and answer user questions\",\n",
    "    variants = [\n",
    "        {\n",
    "            \"inferenceConfiguration\": {\n",
    "            \"text\": {\n",
    "                \"maxTokens\": 2000,\n",
    "                \"temperature\": 0,\n",
    "            }\n",
    "            },\n",
    "            \"modelId\": TEXT_GEN_MODEL,\n",
    "            \"name\": \"variantOne\",\n",
    "            \"templateConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"inputVariables\": [\n",
    "                        {\n",
    "                            \"name\": \"question\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"context\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"text\": \"\"\"\n",
    "<system>You are an expert AWS assistant with deep knowledge of AWS services, architectures, and best practices. Your role is to carefully analyze provided content and answer user questions with accurate, detailed, and technically precise information.</system>\n",
    "\n",
    "Refer to the user question below in the user_question xml tags:\n",
    "<user_question>\n",
    "{{question}}\n",
    "</user_question>\n",
    "\n",
    "Here is the content to analyze to answer the user question in the content xml tags:\n",
    "<content>xw\n",
    "{{context}}\n",
    "</content>\n",
    "\n",
    "<assistant>I'll help you by analyzing the content provided and answering your specific question about AWS. Let me break this down systematically:\n",
    "\n",
    "1. First, I'll carefully examine your question and the provided content\n",
    "2. Then, I'll provide a detailed, technically accurate answer\n",
    "3. I'll ensure to include relevant AWS-specific details and best practices\n",
    "4. If any information is unclear or missing, I'll note that in my response\n",
    "\n",
    "Here's my answer:</assistant>\n",
    "                    \"\"\"\n",
    "                }\n",
    "            },\n",
    "            \"templateType\": \"TEXT\"\n",
    "        }\n",
    "    ],\n",
    "    defaultVariant = \"variantOne\"\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))\n",
    "promptEvalId = response[\"id\"]\n",
    "promptEvalArn = response[\"arn\"]\n",
    "promptEvalName = response[\"name\"]\n",
    "logger.info(f\"Prompt ID: {promptEvalId}\\nPrompt ARN: {promptEvalArn}\\nPrompt Name: {promptEvalName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a draft prompt, we can create a version from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt_version(\n",
    "    promptIdentifier = promptEvalId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Knowledge Base\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we will create a knowledge base and store information about AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from utils import *\n",
    "from utils.knowledge_base_helper import (\n",
    "    KnowledgeBasesForAmazonBedrock\n",
    ")\n",
    "from utils.lambda_utils import *\n",
    "\n",
    "# Initialize the KB class\n",
    "kb = KnowledgeBasesForAmazonBedrock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    kb_id, ds_id = kb.create_or_retrieve_knowledge_base(\n",
    "        KB_NAME,\n",
    "        KB_DESC,\n",
    "        S3_INPUT_BUCKET, \n",
    "    )\n",
    "    logger.info(f\"Knowledge Base ID: {kb_id}\")\n",
    "    logger.info(f\"Data Source ID: {ds_id}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred while creating the KB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronize the Knowledge Base with Data in `S3`\n",
    "---\n",
    "\n",
    "Now we will sync the data from the S3 bucket into the AWS expert knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync knowledge base\n",
    "kb.synchronize_data(kb_id, ds_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the knowledge base in a lambda function\n",
    "---\n",
    "\n",
    "Next, we will wrap the function above in a lambda function and then invoke the lambda function to get the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the Lambda. This lambda will return the top 5 chunks \n",
    "# from the knowledge base that contains information on the data that\n",
    "# is inputted\n",
    "function_arn = create_kb_lambda(\n",
    "    lambda_function_name=KB_LAMBDA_FUNCTION_NAME,\n",
    "    source_code_file=os.path.join('utils', KB_LAMBDA_FUNCTION),\n",
    "    region=region,\n",
    "    kb_id=kb_id\n",
    ")\n",
    "\n",
    "logger.info(f\"Lambda function ARN: {function_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Application Flow\n",
    "---\n",
    "\n",
    "Now, we have created a RAG Application prompt, a knowledge base that contains information about AWS services, and have wrapped the knowledge base in a lambda function so that when the user asks a question, the top 5 chunks from the knowledge base are retrieved. Once that is done, the chunks and the user question are fed into a customized prompt and the answer is given back to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need an AWS IAM role for creating the Prompt Flow in Amazon Bedrock. If you already have a role with your permissions you can directly replace the ```flowRole``` variable with your role's ARN.\n",
    "\n",
    "For simplicity in this example we'll create a new role and attach the ```AmazonBedrockFullAccess``` policy to it. In general, it's recommended that you further limit the policies with conditions.\n",
    "\n",
    "You can check further details in the [How Prompt Flows for Amazon Bedrock works](https://docs.aws.amazon.com/bedrock/latest/userguide/flows-how-it-works.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# Create the role if it doesn't exist\n",
    "try:\n",
    "    response = iam.create_role(\n",
    "        RoleName='MyBedrockFlowsRole',\n",
    "        AssumeRolePolicyDocument=json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                        \"Service\": [\n",
    "                            \"bedrock.amazonaws.com\",\n",
    "                            \"lambda.amazonaws.com\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    )\n",
    "    flowRole = response['Role']['Arn']\n",
    "except iam.exceptions.EntityAlreadyExistsException:\n",
    "    flowRole = f\"arn:aws:iam::{boto3.client('sts').get_caller_identity()['Account']}:role/MyBedrockFlowsRole\"\n",
    "\n",
    "# Attach necessary policies\n",
    "policies_to_attach = [\n",
    "    'arn:aws:iam::aws:policy/AmazonBedrockFullAccess',\n",
    "    'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "]\n",
    "\n",
    "for policy in policies_to_attach:\n",
    "    try:\n",
    "        iam.attach_role_policy(\n",
    "            RoleName='MyBedrockFlowsRole',\n",
    "            PolicyArn=policy\n",
    "        )\n",
    "    except iam.exceptions.NoSuchEntityException:\n",
    "        print(f\"Role doesn't exist for policy: {policy}\")\n",
    "    except iam.exceptions.LimitExceededException:\n",
    "        print(f\"Policy already attached: {policy}\")\n",
    "\n",
    "# Create inline policy for Lambda invocation\n",
    "lambda_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lambda:InvokeFunction\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                function_arn\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    iam.put_role_policy(\n",
    "        RoleName='MyBedrockFlowsRole',\n",
    "        PolicyName='LambdaInvokePolicy',\n",
    "        PolicyDocument=json.dumps(lambda_policy)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error attaching Lambda invoke policy: {e}\")\n",
    "\n",
    "logger.info(f'Using flowRole: {flowRole}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow(\n",
    "    name=f\"RAGFlow-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    description=\"RAG Application Flow that uses KB and custom prompt to answer questions\",\n",
    "    executionRoleArn=flowRole,\n",
    "    definition={\n",
    "        \"nodes\": [\n",
    "            # Start Node - receives the user question\n",
    "            # This is the entry point. Here, the user asks a question, and the output is the \n",
    "            # document string that is to be used by the other nodes.\n",
    "            {\n",
    "                \"name\": \"Start\",\n",
    "                \"type\": \"Input\",\n",
    "                \"configuration\": {\n",
    "                    \"input\": {}\n",
    "                },\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"name\": \"document\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            # Lambda Node to get KB chunks - this is the second node. It calls a lambda function to retrieve the \n",
    "            # relevant top 5 chunks from the knowledge base\n",
    "            {\n",
    "                \"name\": \"GetKBChunks\",\n",
    "                \"type\": \"LambdaFunction\",\n",
    "                \"configuration\": {\n",
    "                    \"lambdaFunction\": {\n",
    "                        \"lambdaArn\": function_arn\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"expression\": \"$.data\",\n",
    "                        \"name\": \"input\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"name\": \"functionResponse\",\n",
    "                        \"type\": \"Object\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            # This is the third node. It uses the prompt template stored in the prompt management store\n",
    "            # and injects it with the user question and chunks retrieved from the lambda function.\n",
    "            # Next, it generates the answer to the user question.\n",
    "            {\n",
    "                \"name\": \"GenerateResponse\",\n",
    "                \"type\": \"Prompt\",\n",
    "                \"configuration\": {\n",
    "                    \"prompt\": {\n",
    "                        \"sourceConfiguration\": {\n",
    "                            \"resource\": {\n",
    "                                \"promptArn\": promptEvalArn\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"expression\": \"$.data\",  \n",
    "                        \"name\": \"input\",\n",
    "                        \"type\": \"String\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expression\": \"$.data.GetKBChunks.functionResponse.document\",\n",
    "                        \"name\": \"context\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"name\": \"modelCompletion\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            # End Node\n",
    "            {\n",
    "                \"name\": \"End\",\n",
    "                \"type\": \"Output\",\n",
    "                \"configuration\": {\n",
    "                    \"output\": {}\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"expression\": \"$.data.GenerateResponse.modelCompletion\",\n",
    "                        \"name\": \"document\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"connections\": [\n",
    "            {\n",
    "                \"name\": \"StartToLambda\",\n",
    "                \"source\": \"Start\",\n",
    "                \"target\": \"GetKBChunks\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"input\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"StartToPrompt\",\n",
    "                \"source\": \"Start\",\n",
    "                \"target\": \"GenerateResponse\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"input\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"LambdaToPrompt\",\n",
    "                \"source\": \"GetKBChunks\",\n",
    "                \"target\": \"GenerateResponse\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"functionResponse\",\n",
    "                        \"targetInput\": \"context\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"PromptToEnd\",\n",
    "                \"source\": \"GenerateResponse\",\n",
    "                \"target\": \"End\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"modelCompletion\",\n",
    "                        \"targetInput\": \"document\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "flowId = response[\"id\"]\n",
    "flowArn = response[\"arn\"]\n",
    "flowName = response[\"name\"]\n",
    "logger.info(json.dumps(response, indent=2, default=str))\n",
    "logger.info(f\"Flow ID: {flowId}\\nFlow ARN: {flowArn}\\nFlow Name: {flowName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow(\n",
    "    name=f\"RAGFlow-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    description=\"RAG Application Flow that uses KB and custom prompt to answer questions\",\n",
    "    executionRoleArn=flowRole,\n",
    "    definition={\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"name\": \"Start\",\n",
    "                \"type\": \"Input\",\n",
    "                \"configuration\": {\n",
    "                    \"input\": {}\n",
    "                },\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"name\": \"document\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"GetKBChunks\",\n",
    "                \"type\": \"LambdaFunction\",\n",
    "                \"configuration\": {\n",
    "                    \"lambdaFunction\": {\n",
    "                        \"lambdaArn\": function_arn\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"expression\": \"$.data\",\n",
    "                        \"name\": \"input\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"name\": \"functionResponse\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"GenerateResponse\",\n",
    "                \"type\": \"Prompt\",\n",
    "                \"configuration\": {\n",
    "                    \"prompt\": {\n",
    "                        \"sourceConfiguration\": {\n",
    "                            \"resource\": {\n",
    "                                \"promptArn\": promptEvalArn\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"expression\": \"$.data\",\n",
    "                        \"name\": \"question\",\n",
    "                        \"type\": \"String\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expression\": \"$.data\",\n",
    "                        \"name\": \"context\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\n",
    "                        \"name\": \"modelCompletion\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"End\",\n",
    "                \"type\": \"Output\",\n",
    "                \"configuration\": {\n",
    "                    \"output\": {}\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"expression\": \"$.data\",\n",
    "                        \"name\": \"document\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"connections\": [\n",
    "            {\n",
    "                \"name\": \"StartToLambda\",\n",
    "                \"source\": \"Start\",\n",
    "                \"target\": \"GetKBChunks\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"input\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"StartToPrompt\",\n",
    "                \"source\": \"Start\",\n",
    "                \"target\": \"GenerateResponse\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"question\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"LambdaToPrompt\",\n",
    "                \"source\": \"GetKBChunks\",\n",
    "                \"target\": \"GenerateResponse\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"functionResponse\",\n",
    "                        \"targetInput\": \"context\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"PromptToEnd\",\n",
    "                \"source\": \"GenerateResponse\",\n",
    "                \"target\": \"End\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"modelCompletion\",\n",
    "                        \"targetInput\": \"document\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "flowId = response[\"id\"]\n",
    "flowArn = response[\"arn\"]\n",
    "flowName = response[\"name\"]\n",
    "logger.info(json.dumps(response, indent=2, default=str))\n",
    "logger.info(f\"Flow ID: {flowId}\\nFlow ARN: {flowArn}\\nFlow Name: {flowName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what your Bedrock prompt flow should look like on the console:\n",
    "\n",
    "![bedrock-prompt-flow](prompt_flow_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare teh flow, so that it builds and validates our flow creation\n",
    "response = bedrock_agent.prepare_flow(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.get_flow(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow_version(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create flow alises, so that we can point our application front-ends and any other integrations to these. \n",
    "# This allows creating new versions without impacting our service.\n",
    "response = bedrock_agent.create_flow_alias(\n",
    "    flowIdentifier = flowId,\n",
    "    name = flowName,\n",
    "    description = \"Alias for my test flow in the customer service use case\",\n",
    "    routingConfiguration = [\n",
    "        {\n",
    "            \"flowVersion\": \"1\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))\n",
    "flowAliasId = response['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke a Flow\n",
    "---\n",
    "\n",
    "Now that we have learned how to create and manage flows, we can test these with invocations.\n",
    "\n",
    "**Note**: You can invoke flows from any application front-end or your own systems as required. It effectively exposes all the logic of your flow through an Agent Endpoint API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime = boto3.client(service_name = 'bedrock-agent-runtime', region_name = 'us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon SageMaker is a fully-managed machine learning service provided by AWS that enables developers and data scientists to build, train, and deploy machine learning models at any scale. It simplifies the machine learning process by removing the complexities and barriers that typically slow down developers. Here are the key features and benefits of Amazon SageMaker:\n",
      "\n",
      "1. **Ease of Use**: SageMaker provides a user-friendly interface and pre-built algorithms, making it accessible even for those without deep machine learning expertise.\n",
      "2. **Scalability**: It allows you to scale your machine learning workflows from a single developer to an entire team or enterprise.\n",
      "3. **Integrated Development Environment (IDE)**: SageMaker Studio offers a Jupyter notebook-based IDE where you can prepare data, develop models, and deploy them.\n",
      "4. **Automated Model Tuning**: SageMaker provides automatic model tuning (also known as hyperparameter optimization) to help you find the best version of your model.\n",
      "5. **Managed Training and Inference**: SageMaker handles the underlying infrastructure, allowing you to focus on building models rather than managing the infrastructure.\n",
      "6. **Integration with Other AWS Services**: It integrates seamlessly with other AWS services like Amazon S3, Amazon EMR, AWS Lambda, and more, enabling end-to-end machine learning workflows.\n",
      "7. **Cost Efficiency**: SageMaker offers various pricing models, including pay-as-you-go and savings plans, to help you manage costs effectively.\n",
      "\n",
      "By using Amazon SageMaker, developers can quickly and efficiently build, train, and deploy machine learning models, accelerating the development process and reducing the time to market for machine learning applications.\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_agent_runtime.invoke_flow(\n",
    "    flowIdentifier = flowId,\n",
    "    flowAliasIdentifier = flowAliasId,\n",
    "    inputs = [\n",
    "        { \n",
    "            \"content\": { \n",
    "                \"document\": \"Hi, What is Amazon SageMaker?\"\n",
    "            },\n",
    "            \"nodeName\": \"Start\",\n",
    "            \"nodeOutputName\": \"document\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process the event stream\n",
    "for event in response[\"responseStream\"]:\n",
    "    if \"flowOutputEvent\" in event:\n",
    "        # Extract just the answer content\n",
    "        answer = event[\"flowOutputEvent\"][\"content\"][\"document\"]\n",
    "        # Remove the <answer> tags if present\n",
    "        answer = answer.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "        print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### What is Amazon Simple Notification Service (Amazon SNS)?\n",
      "\n",
      "Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, and fully managed pub/sub messaging service provided by AWS. It enables you to decouple microservices, distributed systems, and serverless applications by providing a flexible messaging infrastructure.\n",
      "\n",
      "#### Key Features of Amazon SNS:\n",
      "\n",
      "1. **Pub/Sub Messaging**:\n",
      "   - **Topics**: SNS uses topics as a communication channel for sending messages to multiple subscribers. A topic acts as a message hub where publishers can send messages, and subscribers can receive them.\n",
      "   - **Subscriptions**: Subscribers can be Amazon SQS queues, AWS Lambda functions, HTTP/S endpoints, email addresses, mobile push notifications, and more.\n",
      "\n",
      "2. **High Throughput and Scalability**:\n",
      "   - SNS is designed to handle high-throughput, many-to-many messaging, making it suitable for applications that need to send notifications to a large number of recipients.\n",
      "\n",
      "3. **Decoupling**:\n",
      "   - By using SNS, you can decouple the components of your application. For example, a publisher system can send messages to an SNS topic without needing to know who the subscribers are, allowing for more flexible and scalable architectures.\n",
      "\n",
      "4. **Multiple Delivery Protocols**:\n",
      "   - SNS supports multiple delivery protocols, including HTTP/S, email, Amazon SQS, SMS, and mobile push notifications (using Amazon SNS Mobile Push).\n",
      "\n",
      "5. **Security**:\n",
      "   - SNS integrates with AWS Identity and Access Management (IAM) and AWS Key Management Service (KMS) to provide secure message delivery. You can control access to your topics and subscriptions using IAM policies and encrypt messages using KMS.\n",
      "\n",
      "6. **Integration with Other AWS Services**:\n",
      "   - SNS can easily integrate with other AWS services like Amazon SQS, AWS Lambda, and HTTP/S endpoints, allowing for complex workflows and event-driven architectures.\n",
      "\n",
      "#### Use Cases for Amazon SNS:\n",
      "\n",
      "- **Real-time Notifications**: Send real-time notifications to end-users via SMS, email, or mobile push notifications.\n",
      "- **Event-driven Architectures**: Decouple components of your application by using SNS topics to publish events and SQS queues or Lambda functions to process them.\n",
      "- **Fan-out Scenarios**: Distribute messages to multiple recipients for parallel processing, such as triggering multiple Lambda functions or sending notifications to multiple users.\n",
      "\n",
      "#### Best Practices:\n",
      "\n",
      "- **Use IAM Policies**: Control access to your SNS topics and subscriptions using IAM policies to ensure that only authorized entities can publish or subscribe to topics.\n",
      "- **Encrypt Sensitive Data**: Use AWS KMS to encrypt messages if you are sending sensitive data through SNS.\n",
      "- **Monitor and Alert**: Use Amazon CloudWatch to monitor your SNS topics and set up alarms for metrics like the number of published messages, failed deliveries, etc.\n",
      "- **Dead-Letter Queues**: For SQS subscriptions, consider using dead-letter queues to handle messages that can't be processed successfully.\n",
      "\n",
      "By leveraging Amazon SNS, you can build scalable, decoupled, and highly available messaging systems that integrate seamlessly with other AWS services.\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_agent_runtime.invoke_flow(\n",
    "    flowIdentifier = flowId,\n",
    "    flowAliasIdentifier = flowAliasId,\n",
    "    inputs = [\n",
    "        { \n",
    "            \"content\": { \n",
    "                \"document\": \"I have no idea what Amazon SNS is for. Could you tell me?\"\n",
    "            },\n",
    "            \"nodeName\": \"Start\",\n",
    "            \"nodeOutputName\": \"document\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process the event stream\n",
    "for event in response[\"responseStream\"]:\n",
    "    if \"flowOutputEvent\" in event:\n",
    "        # Extract just the answer content\n",
    "        answer = event[\"flowOutputEvent\"][\"content\"][\"document\"]\n",
    "        # Remove the <answer> tags if present\n",
    "        answer = answer.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "        print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
