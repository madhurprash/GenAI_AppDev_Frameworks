{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Prompt Flows\n",
    "---\n",
    "\n",
    "Amazon Bedrock Prompt flows offers the ability for you to use supported foundation models (FMs) to build workflows by linking prompts, foundational models, and other AWS services to create end-to-end solutions.\n",
    "\n",
    "With prompt flows, you can quickly build complex generative AI workflows using a visual builder, easily integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and other AWS services such as AWS Lambda by transferring data between them, and deploying immutable workflows to move from testing to production in few clicks.\n",
    "\n",
    "#### Create the prompt flow:\n",
    "1. Specify a prompt flow name, description, and appropriate IAM permissions.\n",
    "\n",
    "2. Design your prompt flow by deciding the nodes you want to use.\n",
    "\n",
    "3. Create or define all the resources you require for each node. For example, if you are planning to use an AWS Lambda function, define the functions you need for the node to complete its task.\n",
    "\n",
    "4. Add nodes to your prompt flow, configure them, and create connections between the nodes by linking the output of a node to the input of another node in the prompt flow.\n",
    "\n",
    "#### Test the prompt flow:\n",
    "1. Prepare the prompt flow, so that the latest changes apply to the working draft of the prompt flow, a version of the prompt flow that you can use to iteratively test and update your prompt flow\n",
    "\n",
    "2. Test the prompt flow by invoking it with sample inputs to see the outputs it yields.\n",
    "\n",
    "3. When you're satisfied with a prompt flow's configuration, you can create a snapshot of it by publishing a version. The version preserves prompt flow definition as it exists at the time of the creation. Versions are immutable because they act as a snapshot of the prompt flow at the time it was created.\n",
    "\n",
    "#### Deploy the prompt flow\n",
    "1. Create an alias that points to the version of your prompt flow that you want to use in your application.\n",
    "\n",
    "2. Set up your application to make InvokeFlow requests to the alias. If you need to revert to an older version or upgrade to a newer one, you can change the routing configuration of the alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "BEDROCK_HAIKU_MODELID: str = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "# The prompt management and flows features are part of the bedrock agent SDK\n",
    "bedrock_agent = boto3.client(service_name=\"bedrock-agent\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Amazon Bedrock Prompt Management_ streamlines the creation, evaluation, deployment, and sharing of prompts in the Amazon Bedrock console and via APIs in the SDK. This feature helps developers and business users obtain the best responses from foundation models for their specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template: str = \"\"\"You're a customer service agent for the e-commerce company Octank. \n",
    "                    Answer the following user query in a friendly and direct way: {{input}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt(\n",
    "    name = f\"MyTestPrompt-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    description = \"This is my test prompt for the customer service use case\",\n",
    "    variants = [\n",
    "        {\n",
    "            \"inferenceConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"maxTokens\": 3000,\n",
    "                    \"temperature\": 0,\n",
    "                    \"topP\": 0.999,\n",
    "                }\n",
    "            },\n",
    "            \"modelId\": BEDROCK_HAIKU_MODELID,\n",
    "            \"name\": \"variant-001\",\n",
    "            \"templateConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"inputVariables\": [\n",
    "                        {\n",
    "                            \"name\": \"input\"\n",
    "                        }\n",
    "\n",
    "                    ],\n",
    "                    \"text\": prompt_template\n",
    "                }\n",
    "            },\n",
    "            \"templateType\": \"TEXT\"\n",
    "        }\n",
    "    ],\n",
    "    defaultVariant = \"variant-001\"\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "promptId = response[\"id\"]\n",
    "promptArn = response[\"arn\"]\n",
    "promptName = response[\"name\"]\n",
    "print(f\"Prompt ID: {promptId}\\nPrompt ARN: {promptArn}\\nPrompt Name: {promptName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have a draft prompt, we can create versions from it\n",
    "response = bedrock_agent.create_prompt_version(\n",
    "    promptIdentifier = promptId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of prompts in our Prompt Library or catalog\n",
    "response = bedrock_agent.list_prompts(\n",
    "    maxResults = 10\n",
    ")\n",
    "print(json.dumps(response[\"promptSummaries\"], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read details about a prompt\n",
    "response = bedrock_agent.get_prompt(\n",
    "    promptIdentifier = promptId,\n",
    "    promptVersion = \"1\"\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Flows\n",
    "Now that we've learned how to create and manage prompts, we can continue exploring how to build generative AI applications logic by creating workflows. For this, we'll rely on Prompt Flows for Amazon Bedrock.\n",
    "\n",
    "### Create and Manage Flows\n",
    "Let's create a simple flow that will load a prompt from our catalog. Note you can also create more complex flows involving chaining of steps, and conditions for dynamically routing, but let's keep it simple for now.\n",
    "\n",
    "_Pre-requisite: For using Flows you need to make sure you have the proper AWS IAM permissions in place. You can check details in the How Prompt Flows for Amazon Bedrock works documentation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_role: str = \"arn:aws:iam::015469603702:role/prompt_flow_role\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow(\n",
    "    name = f\"MyTestFlow-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    description = \"This is my test flow for the customer service use case\",\n",
    "    executionRoleArn = flow_role,\n",
    "    definition = {\n",
    "      \"nodes\": [\n",
    "          {\n",
    "              \"name\": \"StartNode\",\n",
    "              \"type\": \"Input\",\n",
    "              \"configuration\": {\n",
    "                  \"input\": {}\n",
    "              },\n",
    "              \"outputs\": [\n",
    "                  {\n",
    "                      \"name\": \"document\",\n",
    "                      \"type\": \"String\"\n",
    "                  }\n",
    "              ],\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"Prompt_1\",\n",
    "            \"type\": \"Prompt\",\n",
    "            \"configuration\": {\n",
    "              \"prompt\": {\n",
    "                \"sourceConfiguration\": {\n",
    "                  \"resource\": {\n",
    "                      \"promptArn\": promptArn\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"inputs\": [\n",
    "              {\n",
    "                \"expression\": \"$.data\",\n",
    "                \"name\": \"input\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "              {\n",
    "                \"name\": \"modelCompletion\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"EndNode\",\n",
    "            \"type\": \"Output\",\n",
    "            \"configuration\": {\n",
    "                \"output\": {}\n",
    "            },\n",
    "            \"inputs\": [\n",
    "              {\n",
    "                \"expression\": \"$.data\",\n",
    "                \"name\": \"document\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "          }\n",
    "      ],\n",
    "      \"connections\": [\n",
    "          {\n",
    "              \"name\": \"Connection_1\",\n",
    "              \"source\": \"StartNode\",\n",
    "              \"target\": \"Prompt_1\",\n",
    "              \"type\": \"Data\",\n",
    "              \"configuration\":{\n",
    "                  \"data\": {\n",
    "                      \"sourceOutput\": \"document\",\n",
    "                      \"targetInput\": \"input\"\n",
    "                  }\n",
    "              }\n",
    "          },\n",
    "          {\n",
    "              \"name\": \"Connection_2\",\n",
    "              \"source\": \"Prompt_1\",\n",
    "              \"target\": \"EndNode\",\n",
    "              \"type\": \"Data\",\n",
    "              \"configuration\": {\n",
    "                  \"data\": {\n",
    "                      \"sourceOutput\": \"modelCompletion\",\n",
    "                      \"targetInput\": \"document\"\n",
    "                  }\n",
    "              }\n",
    "          }\n",
    "      ],\n",
    "    }\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "flowId = response[\"id\"]\n",
    "flowArn = response[\"arn\"]\n",
    "flowName = response[\"name\"]\n",
    "print(f\"Flow ID: {flowId}\\nFlow ARN: {flowArn}\\nFlow Name: {flowName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our first flow, we can prepare it. This basically builds and validates our flow.\n",
    "response = bedrock_agent.prepare_flow(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.get_flow(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow_version(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow_alias(\n",
    "    flowIdentifier = flowId,\n",
    "    name = flowName,\n",
    "    description = \"Alias for my test flow in the customer service use case\",\n",
    "    routingConfiguration = [\n",
    "        {\n",
    "            \"flowVersion\": \"1\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "flowAliasId = response['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.update_flow_alias(\n",
    "    flowIdentifier = flowId,\n",
    "    aliasIdentifier = flowAliasId,\n",
    "    name = flowName,\n",
    "    routingConfiguration = [\n",
    "        {\n",
    "            \"flowVersion\": \"1\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "flowAliasId = response[\"id\"]\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime = boto3.client(service_name = 'bedrock-agent-runtime', region_name = 'us-east-1')\n",
    "# invoke a flow\n",
    "response = bedrock_agent_runtime.invoke_flow(\n",
    "    flowIdentifier = flowId,\n",
    "    flowAliasIdentifier = flowAliasId,\n",
    "    inputs = [\n",
    "        { \n",
    "            \"content\": { \n",
    "                \"document\": \"Hi, I need help with my order!\"\n",
    "            },\n",
    "            \"nodeName\": \"StartNode\",\n",
    "            \"nodeOutputName\": \"document\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "event_stream = response[\"responseStream\"]\n",
    "for event in event_stream:\n",
    "    print(json.dumps(event, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
