{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate prompt engineering with DSPy and Haystack\n",
    "---\n",
    "\n",
    "In this example, we will take a look at how we can optimize and automate the prompt engineering process using `Haystack` (to create our generative AI pipeline) and `DSPy` (to optimize our prompts and LM weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each component, it is essential to know the names of the input and the output. There are several components for various steps\n",
    "# of the generative AI pipeline that are provided out of the box. This includes components for document stores, embedders, prompt\n",
    "# builders and generators. You can also build your own component.\n",
    "!pip install -Uq pip\n",
    "!pip install -Uq haystack\n",
    "!pip install -Uq sentence-transformers\n",
    "!pip install -Uq amazon-bedrock-haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "import logging\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import globals as g\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the existing haystack pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Create temporary file and download pipeline\n",
    "with tempfile.NamedTemporaryFile(mode='wb', suffix='.yml', delete=False) as tmp_file:\n",
    "    # Download from the same location where we uploaded\n",
    "    s3_client.download_fileobj(\n",
    "        g.HAYSTACK_PIPELINE_BUCKET, \n",
    "        g.HAYSTACK_PIPELINE_KEY, \n",
    "        tmp_file\n",
    "    )\n",
    "    logger.info(f\"Downloaded the haystack pipeline from {g.HAYSTACK_PIPELINE_BUCKET}/{g.HAYSTACK_PIPELINE_KEY} to {tmp_file.name}\")\n",
    "    tmp_file_path = tmp_file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load document store\n",
    "doc_store_key = \"pipelines/document_store/documents.json\"\n",
    "with tempfile.NamedTemporaryFile(mode='wb', suffix='.pkl', delete=False) as tmp_file:\n",
    "    s3_client.download_fileobj(\n",
    "        g.HAYSTACK_PIPELINE_BUCKET, \n",
    "        doc_store_key, \n",
    "        tmp_file\n",
    "    )\n",
    "    print(f\"Downloaded the document store from {g.HAYSTACK_PIPELINE_BUCKET}/{doc_store_key} to {tmp_file.name}\")\n",
    "    docstore_tmp_path = tmp_file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "# Now we will load the pipeline from the temporary file path\n",
    "with open(tmp_file_path, 'r') as file:\n",
    "    loaded_pipeline = Pipeline.load(file)\n",
    "    print(f\"Loaded the haystack pipeline from {tmp_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will clean up the temporary file path and then see the contents of the pipeline\n",
    "# that we had saved to s3\n",
    "os.remove(tmp_file_path)\n",
    "logger.info(\"Loaded Pipeline Structure:\")\n",
    "loaded_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents stored earlier and get those documents in the doc store\n",
    "import json\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "\n",
    "# Load the documents from the JSON file\n",
    "with open(docstore_tmp_path, 'r') as f:\n",
    "    documents_dicts = json.load(f)\n",
    "\n",
    "from haystack import Document\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings back to NumPy arrays\n",
    "for doc_dict in documents_dicts:\n",
    "    if 'embedding' in doc_dict and isinstance(doc_dict['embedding'], list):\n",
    "        doc_dict['embedding'] = np.array(doc_dict['embedding'])\n",
    "\n",
    "# Reconstruct Document objects\n",
    "documents = [Document.from_dict(doc_dict) for doc_dict in documents_dicts]\n",
    "document_store = ChromaDocumentStore()\n",
    "# Write the documents to the document store\n",
    "document_store.write_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question: str = \"What is a neurodegenerative disease? Give me examples\"\n",
    "document_store = loaded_pipeline.get_component('retriever')\n",
    "# Directly query the document store to simulate what the BM25 retriever would retrieve\n",
    "retrieved_docs = document_store.run(question) \n",
    "\n",
    "# Print the retrieved documents' content or metadata\n",
    "print(f\"Retrieved Documents: {retrieved_docs}\")\n",
    "\n",
    "# Now proceed with the full pipeline run to generate the final response\n",
    "response = loaded_pipeline.run({\n",
    "    \"retriever\": {\"query\": question},\n",
    "    \"prompt_builder\": {\"question\": question}\n",
    "})\n",
    "print(\"LLM Response:\")\n",
    "print(response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a simple RAG application working. Now we want to optimize the prompts so that we can get more concise responses. We will achieve this using `DSPy`\n",
    "\n",
    "`DSPy` is a framework designed to algorithmically optimize prompts for Language Models.\n",
    "\n",
    "### Use DSPy to get concise answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.primitives.prediction import Prediction\n",
    "\n",
    "\n",
    "lm = dspy.LM(g.BEDROCK_HAIKU_MODELID)\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dspy signature for question answering\n",
    "---\n",
    "\n",
    "We can make changes in the signature to create shorter outputs to questions as we desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create a signature to generate answers that are more concise\n",
    "# and capture the key components of the question instead of giving lengthy\n",
    "# responses\n",
    "# Enhanced signature for generating concise responses\n",
    "class GenerateConciseResponses(dspy.Signature):\n",
    "    \"\"\"Generate extremely concise, information-dense responses to questions along with examples.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"Relevant information to answer the question\")\n",
    "    question = dspy.InputField()\n",
    "    response = dspy.OutputField(desc=\"Provide a concise answer in 15 words or less. Focus on key information only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RAG module to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConciseRAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateConciseResponses)\n",
    "\n",
    "    def retrieve(self, question):\n",
    "        \"\"\"\n",
    "        In this function, we make this so it is possible for us to \n",
    "        re use the haystack retriever that already contains all of the indexed \n",
    "        documents\n",
    "        \"\"\"\n",
    "        \n",
    "        results = document_store.run(query=question)\n",
    "        passages = [res.content for res in results['documents']]\n",
    "        return Prediction(passages=passages)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and test data\n",
    "---\n",
    "\n",
    "Now we will refer to the previous HF dataset we loaded, and use it to split into training and test examples. We will use these examples in our prompt optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"vblagoje/PubMedQA_instruction\")\n",
    "# Shuffle and select 1000 samples from the 'train' split\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "training_examples: List = []\n",
    "test_examples: List = []\n",
    "# Access the train split specifically\n",
    "train_data = dataset['train']\n",
    "\n",
    "for i, example in enumerate(train_data):\n",
    "    try:\n",
    "        # craft the dspy example\n",
    "        dspy_example = dspy.Example(question=example['instruction'], answer=example['response']).with_inputs('question')\n",
    "        if i < 20:\n",
    "            training_examples.append(dspy_example)\n",
    "        elif i < 70:\n",
    "            test_examples.append(dspy_example)\n",
    "        else:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample training example:\")\n",
    "print(f\"Question: {training_examples[0].question}\")\n",
    "print(f\"Answer: {training_examples[0].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a metric to optimize for\n",
    "---\n",
    "\n",
    "In this section, we will add a metric. Adding and tracking a metric as a part of the prompt optimization process is essential. We will create an example metric to use the HayStack SAS evaluator for the semantic match.\n",
    "\n",
    "1. `concise_metric` checks for the semantic similarity\n",
    "1. Uses a custom tiering system to score the length of the outputs.\n",
    "1. Gives the final score based on the tiered length * semantic similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.evaluators import SASEvaluator\n",
    "\n",
    "# Initialize the sas evaluator and warm it up\n",
    "sas_evaluator = SASEvaluator()\n",
    "sas_evaluator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concise_metric(example, pred, trace=None) -> float:\n",
    "    \"\"\"\n",
    "    Enhanced metric that strongly favors concise, information-dense responses.\n",
    "    \"\"\"\n",
    "    semantic_similarity = sas_evaluator.run(\n",
    "        ground_truth_answers=[example.answer], \n",
    "        predicted_answers=[pred.answer]\n",
    "    )[\"score\"]\n",
    "    \n",
    "    pred_words = len(pred.answer.split())\n",
    "    target_words = len(example.answer.split())\n",
    "\n",
    "    # Enhanced tiered scoring with information density consideration\n",
    "    if pred_words <= 20:\n",
    "        length_multiplier = 1.2 \n",
    "    elif pred_words <= 25:\n",
    "        length_multiplier = 1.0\n",
    "    elif pred_words <= 30:\n",
    "        length_multiplier = 0.7\n",
    "    elif pred_words <= 35:\n",
    "        length_multiplier = 0.4\n",
    "    else:\n",
    "        length_multiplier = 0.1\n",
    "    \n",
    "    # Penalize responses that are much shorter than target (might be incomplete)\n",
    "    if pred_words < target_words * 0.5:\n",
    "        completeness_penalty = 0.3\n",
    "    else:\n",
    "        completeness_penalty = 0.0\n",
    "    final_score = (semantic_similarity * length_multiplier) - completeness_penalty\n",
    "    # Normalize score to be between 0 and 1\n",
    "    return max(min(final_score, 1.0), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for how the uncompiled ConciseRAG module performs on the dataset before any optimizations\n",
    "uncompiled_rag = ConciseRAG()\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(\n",
    "    metric=concise_metric, devset=test_examples, num_threads=1, display_progress=True, display_table=5\n",
    ")\n",
    "evaluate(uncompiled_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the metric was evaluated as 42.35, which is pretty low. Now that we have an idea of what the overall dataset it like, we can compile the `ConciseRAG` module using several optimization metrics.\n",
    "\n",
    "Here, we will use the `BootstrapFewShot` teleprompter/optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin module compilation with the `BootstrapFewShot` Optimizer\n",
    "---\n",
    "\n",
    "In this example, we will compile our `ConciseRAG()` module with `BootstrapFewShot` optimizer from DSPy (~3 minutes)\n",
    "\n",
    "View more about this optimizer here: https://dspy.ai/deep-dive/optimizers/bootstrap-fewshot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "optimizer = BootstrapFewShot(metric=concise_metric)\n",
    "\n",
    "# Now we will compile our RAG module\n",
    "compiled_rag = optimizer.compile(ConciseRAG(), trainset=training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once our module is compiled, let's re-evaluate all of the prompts\n",
    "evaluate = Evaluate(\n",
    "    metric=concise_metric, devset=test_examples, num_threads=1, display_progress=True, display_table=5\n",
    ")\n",
    "evaluate(compiled_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a significant jump in the metric from `26.82` to `47.96`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import contextlib\n",
    "\n",
    "# Step 1: Capture `lm.inspect_history()` output to a string\n",
    "output_buffer = io.StringIO()\n",
    "with contextlib.redirect_stdout(output_buffer):\n",
    "    lm.inspect_history(n=1)\n",
    "captured_output = output_buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.builders import PromptBuilder, AnswerBuilder\n",
    "from haystack import Pipeline\n",
    "from haystack_integrations.components.generators.amazon_bedrock import AmazonBedrockGenerator\n",
    "\n",
    "# Initialize the amazon bedrock generator that is used to generate responses to questions\n",
    "# at the inference step of the RAG pipeline\n",
    "new_generator = AmazonBedrockGenerator(model=BEDROCK_HAIKU_MODELID)\n",
    "\n",
    "\n",
    "template = captured_output +\"\"\"\n",
    "---\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    «{{ document.content }}»\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Reasoning: Let's think step by step in order to\n",
    "\"\"\"\n",
    "\n",
    "new_prompt_builder = PromptBuilder(template=template)\n",
    "\n",
    "new_retriever = InMemoryBM25Retriever(document_store, top_k=3)\n",
    "\n",
    "answer_builder = AnswerBuilder(pattern=\"Answer: (.*)\")\n",
    "\n",
    "\n",
    "optimized_rag_pipeline = Pipeline()\n",
    "optimized_rag_pipeline.add_component(\"retriever\", new_retriever)\n",
    "optimized_rag_pipeline.add_component(\"prompt_builder\", new_prompt_builder)\n",
    "optimized_rag_pipeline.add_component(\"llm\", new_generator)\n",
    "optimized_rag_pipeline.add_component(\"answer_builder\", answer_builder)\n",
    "\n",
    "optimized_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "optimized_rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "optimized_rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_rag_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a neurodegenerative disease? Give me examples\"\n",
    "response = optimized_rag_pipeline.run({\"retriever\": {\"query\": question}, \"prompt_builder\": {\"question\": question}, \"answer_builder\": {\"query\": question}})\n",
    "\n",
    "print(response[\"answer_builder\"][\"answers\"][0].data) # to do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
