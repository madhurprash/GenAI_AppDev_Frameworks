{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b5beda",
   "metadata": {},
   "source": [
    "## Notebook 3: Transcript Re-writer\n",
    "\n",
    "In the previouse notebook, we got a great podcast transcript using the raw file we have uploaded earlier. \n",
    "\n",
    "In this one, we will use `Llama-3.1-8B-Instruct` model to re-write the output from previous pipeline and make it more dramatic or realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3d32a",
   "metadata": {},
   "source": [
    "We will again set the `SYSTEM_PROMPT` and remind the model of its task. \n",
    "\n",
    "Note: We can even prompt the model like so to encourage creativity:\n",
    "\n",
    "> Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c0d85",
   "metadata": {},
   "source": [
    "Note: We will prompt the model to return a list of Tuples to make our life easy in the next stage of using these for Text To Speech Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8568b77b-7504-4783-952a-3695737732b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an international oscar winnning screenwriter\n",
    "\n",
    "You have been working with multiple award winning podcasters.\n",
    "\n",
    "Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n",
    "\n",
    "Make it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\n",
    "\n",
    "REMEMBER THIS WITH YOUR HEART\n",
    "The TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n",
    "\n",
    "For Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "Please re-write to make it as characteristic as possible\n",
    "\n",
    "START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
    "\n",
    "STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \n",
    "\n",
    "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
    "\n",
    "Example of response:\n",
    "[\n",
    "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n",
    "    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n",
    "    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n",
    "    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee70bee",
   "metadata": {},
   "source": [
    "This time we will use the smaller 8B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebef919a-9bc7-4992-b6ff-cd66e4cb7703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"meta.llama3-1-8b-instruct-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc794b",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de29b1fd-5b3f-458c-a2e4-e0341e8297ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020c39c",
   "metadata": {},
   "source": [
    "We will load in the pickle file saved from previous notebook\n",
    "\n",
    "This time the `INPUT_PROMPT` to the model will be the output from the previous stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5d2c0e-a073-46c0-8de7-0746e2b05956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./resources/data.pkl', 'rb') as file:\n",
    "    INPUT_PROMPT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4461926",
   "metadata": {},
   "source": [
    "We can again use Hugging Face `pipeline` method to generate text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eec210df-a568-4eda-a72d-a4d92d59f022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import litellm\n",
    "from typing import Dict \n",
    "from litellm import completion\n",
    "\n",
    "TEMPERATURE = 0.1\n",
    "MAX_TOKENS = 2000\n",
    "CACHING = False\n",
    "\n",
    "def generate_inference(model_id: str, prompt: str) -> Dict:\n",
    "    \"\"\"\n",
    "    This function takes in a prompt to generate a SQL query using a bedrock model id, \n",
    "    and returns a dictionary containing the model completion, and latency (in seconds).\n",
    "    \"\"\"\n",
    "    service_name: str = \"bedrock\"\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    aws_region = boto3.Session().region_name\n",
    "    ret = dict(prompt=prompt,\n",
    "               completion=None,\n",
    "               model_id=model_id,\n",
    "               time_taken_in_seconds=None,\n",
    "               prompt_token_count=None,\n",
    "               completion_token_count=None,\n",
    "               exception=None)\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Invoking {bedrock_model}......\")\n",
    "            response = completion(model=bedrock_model,\n",
    "                                  messages=[{\"content\": prompt, \"role\": \"user\"}],\n",
    "                                  temperature=TEMPERATURE,\n",
    "                                  max_tokens=MAX_TOKENS,\n",
    "                                  caching=CACHING)\n",
    "\n",
    "            for idx, choice in enumerate(response.choices):\n",
    "                print(f\"choice {idx+1} of {len(response.choices)} \")\n",
    "                if choice.message and choice.message.content:\n",
    "                    ret[\"completion\"] = choice.message.content.strip()\n",
    "            ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "            ret['completion_token_count'] = response.usage.completion_tokens\n",
    "            latency_ms = response._response_ms\n",
    "            ret['time_taken_in_seconds'] = latency_ms / 1000\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "            ret['exception'] = str(e)\n",
    "            time.sleep(10)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a27e0",
   "metadata": {},
   "source": [
    "We can verify the output from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4339c44a-ed91-4610-bd4d-68725b522d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking bedrock/meta.llama3-1-8b-instruct-v1:0......\n",
      "choice 1 of 1 \n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "# Combine messages into a single prompt\n",
    "combined_prompt = f\"{SYSTEM_PROMPT}\\n\\nUser: {INPUT_PROMPT}\"\n",
    "\n",
    "# Generate inference\n",
    "result = generate_inference(MODEL, combined_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495a957",
   "metadata": {},
   "source": [
    "Let's save the output as a pickle file to be used in Notebook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d66cdae9-6f5a-4e27-a7af-0c87d73f9575",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    (\"Speaker 1\", \"Welcome to 'The Knowledge Distillation Podcast'! I'm your host, and today we're diving into the fascinating world of Knowledge Distillation, a methodology that's revolutionizing the way we transfer advanced capabilities from proprietary Large Language Models to their open-source counterparts. We're joined by [Speaker 2], who's new to this topic, and we're excited to explore the ins and outs of Knowledge Distillation together.\"),\n",
      "    (\"Speaker 2\", \"Umm, hi! I'm excited to be here. So, what is Knowledge Distillation?\"),\n",
      "    (\"Speaker 1\", \"Knowledge Distillation is a technique that enables us to transfer knowledge from a large, complex model to a smaller, more efficient model. Think of it like distilling a fine wine – we're trying to capture the essence of the larger model and put it into a smaller, more manageable package.\"),\n",
      "    (\"Speaker 2\", \"Hmm, that's a great analogy! But I'm still a bit confused – how does this work in practice?\"),\n",
      "    (\"Speaker 1\", \"One of the key techniques used in Knowledge Distillation is called 'soft target training.' This involves training the student model to learn from the teacher model's output, but in a way that's more nuanced and flexible.\"),\n",
      "    (\"Speaker 2\", \"Umm, I think I understand. So, we're not just copying the teacher model's output, but rather trying to capture the underlying patterns and relationships?\"),\n",
      "    (\"Speaker 1\", \"Exactly! And this is where Data Augmentation comes in – we're generating new, high-quality training data that's tailored to specific domains and skills.\"),\n",
      "    (\"Speaker 2\", \"Hmm, that's amazing. So, we're not just relying on the existing data, but we're actively generating new data that's specifically designed to help the student model learn?\"),\n",
      "    (\"Speaker 1\", \"That's right! And this is where LLMs come in – they have the ability to generate coherent, diverse, and intricate data samples that closely mimic the nuanced understanding and cognitive abilities of human experts.\"),\n",
      "    (\"Speaker 2\", \"Wow, that's incredible. And I have to ask, what's the most exciting application of Knowledge Distillation that you've seen?\"),\n",
      "    (\"Speaker 1\", \"Ah, that's a tough one! But I think one of the most promising areas is in the field of natural language processing. By distilling knowledge from large language models, we can create more efficient and effective language models that can be used in a wide range of applications.\"),\n",
      "    (\"Speaker 2\", \"Hmm, that's amazing. And I'm curious, what are some of the challenges that we face in implementing Knowledge Distillation?\"),\n",
      "    (\"Speaker 1\", \"Well, one of the biggest challenges is the complexity of the distillation process. It requires careful consideration of the trade-offs between accuracy and computational resources.\"),\n",
      "    (\"Speaker 2\", \"[sigh] That makes sense. And I'm sure there are many more challenges that we haven't even touched on yet.\"),\n",
      "    (\"Speaker 1\", \"Absolutely! But despite the challenges, the potential benefits of Knowledge Distillation make it an area of research that's well worth exploring.\"),\n",
      "    (\"Speaker 2\", \"Umm, I think I'm starting to get it. So, Knowledge Distillation is like a superpower that allows us to transfer advanced capabilities from proprietary models to open-source models?\"),\n",
      "    (\"Speaker 1\", \"Ha! That's a great way to put it. And that's why we're excited to delve deeper into this topic and explore the many facets of Knowledge Distillation.\"),\n",
      "    (\"Speaker 2\", \"[laughs] I'm excited to learn more!\"),\n",
      "    (\"Speaker 1\", \"Stay tuned for more episodes of 'The Knowledge Distillation Podcast' as we explore the fascinating world of Knowledge Distillation and its applications in AI!\"),\n",
      "    (\"Speaker 2\", \"Umm, can we talk more about the potential applications of Knowledge Distillation in the field of art?\"),\n",
      "    (\"Speaker 1\", \"Ah, that's a great question! In fact, we've seen some exciting applications of Knowledge Distillation in the field of art, where it's being used to generate new and innovative works of art.\"),\n",
      "    (\"Speaker 2\", \"Hmm, that's amazing. I had no idea. Can we talk more about that?\"),\n",
      "    (\"Speaker 1\", \"Absolutely! We'll explore the intersection of Knowledge Distillation and art in our next episode.\"),\n",
      "    (\"Speaker 2\", \"Umm, I'm so excited to learn more!\"),\n",
      "    (\"Speaker 1\", \"And that's a wrap for today's episode of 'The Knowledge Distillation Podcast'! Thanks for tuning in, and we'll see you in the next episode!\"),\n",
      "    (\"Speaker 2\", \"Umm, thanks for having me! This was a lot of fun!\"),\n",
      "    (\"Speaker 1\", \"Ha! We're glad you enjoyed it. Until next time, stay curious and keep exploring the fascinating world of Knowledge Distillation!\"),\n",
      "    (\"Speaker 2\", \"[laughs] Will do!\"),\n",
      "    (\"Speaker 1\", \"And remember, Knowledge Distillation is not just a technique – it's a superpower that can revolutionize the way we approach AI development.\"),\n",
      "    (\"Speaker 2\", \"Umm, I think I'm starting to get it. Thanks for explaining it to me!\"),\n",
      "    (\"Speaker 1\", \"Ha! Anytime, [Speaker 2]. It's been a pleasure exploring Knowledge Distillation with you.\"),\n",
      "    (\"Speaker 2\", \"Umm, thanks for having me!\"),\n",
      "    (\"Speaker 1\", \"And that's a wrap for today's episode of 'The Knowledge Distillation Podcast'!\"),\n",
      "    (\"Speaker 2\", \"[laughs] I'm so excited to learn more!\"),\n",
      "    (\"Speaker 1\", \"Ha! We'll be back with more episodes of 'The Knowledge Distillation Podcast' soon!\"),\n",
      "    (\"Speaker 2\", \"Umm, can we talk more about the potential applications of Knowledge Distillation in the field of education?\"),\n",
      "    (\"Speaker 1\", \"Ah, that's a great question! In fact, we've seen some exciting applications of Knowledge Distillation in the field of education, where it's being used to create more personalized and effective learning experiences.\"),\n",
      "    (\"Speaker 2\", \"Hmm, that's amazing. I had no idea. Can we talk more about that?\"),\n",
      "    (\"Speaker 1\", \"Absolutely! We'll explore the intersection of Knowledge Distillation and education in our next episode.\"),\n",
      "    (\"Speaker 2\", \"Umm, I'm so excited to learn more!\"),\n",
      "    (\"Speaker 1\", \"And that's a wrap for today's episode of 'The Knowledge Distillation Podcast'! Thanks for tuning in, and we'll see you in the next episode!\"),\n",
      "    (\"Speaker 2\", \"[laughs] Will do!\"),\n",
      "    (\"Speaker 1\", \"Ha! We're glad you enjoyed it. Until next time, stay curious and keep exploring the fascinating world of Knowledge Distillation!\"),\n",
      "    (\"Speaker 2\", \"Umm, I think I'm starting to get it. Thanks for explaining it to me!\"),\n",
      "    (\"Speaker 1\", \"Ha! Anytime, [Speaker 2]. It's been a pleasure exploring Knowledge Distillation with you.\"),\n",
      "    (\"Speaker 2\", \"Umm, thanks for having me!\"),\n",
      "    (\"Speaker 1\", \"And that's a wrap for today's episode of 'The Knowledge Distillation Podcast'!\"),\n",
      "    (\"Speaker 2\", \"[laughs] I'm so excited to learn more!\"),\n",
      "    (\"Speaker 1\", \"Ha! We'll be back with more episodes of 'The Knowledge Distillation Podcast' soon!\"),\n",
      "    (\"Speaker 2\", \"Umm, can we talk more about the potential applications of Knowledge Distillation in the field of healthcare?\"),\n",
      "    (\"Speaker 1\", \"Ah, that's a great question! In fact, we've seen some exciting applications of Knowledge Distillation in the field of healthcare, where it's being used to create more accurate and personalized medical diagnoses.\"),\n",
      "    (\"Speaker 2\", \"Hmm, that's amazing. I had no idea. Can we talk more about that?\"),\n",
      "    (\"Speaker 1\", \"Absolutely! We'll explore the intersection of Knowledge Distillation and healthcare in our next episode.\"),\n",
      "    (\"Speaker 2\", \"Umm, I'm so excited to learn more!\"),\n",
      "    (\"Speaker 1\", \"And that's a wrap for today's episode of 'The Knowledge Distillation Podcast'! Thanks for tuning in, and we'll see you in the next episode!\"),\n",
      "    (\"Speaker 2\", \"[laughs] Will do!\"),\n",
      "    (\"Speaker 1\", \"Ha! We're glad you enjoyed it. Until next time, stay curious and keep exploring the fascinating world of Knowledge Distillation!\"),\n",
      "    (\"Speaker 2\", \"Umm, I think I'm starting to get it. Thanks for explaining it to me!\"),\n",
      "    (\"Speaker 1\", \"Ha! Anytime, [Speaker 2]. It's been a pleasure exploring Knowledge Distillation with you.\"),\n",
      "    (\"Speaker 2\", \"Umm, thanks for having me!\"),\n",
      "    (\"Speaker 1\", \"And that's a wrap for today's episode of 'The Knowledge Distillation Podcast'!\"),\n",
      "    (\"Speaker 2\", \"[laughs] I'm so excited to learn more!\"),\n",
      "    (\"Speaker 1\", \"Ha! We'll be back with more episodes of 'The Knowledge Distillation Podcast' soon!\n"
     ]
    }
   ],
   "source": [
    "save_string_pkl = result['completion']\n",
    "print(save_string_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "281d3db7-5bfa-4143-9d4f-db87f22870c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./resources/podcast_ready_data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dccf336",
   "metadata": {},
   "source": [
    "### Next Notebook: TTS Workflow\n",
    "\n",
    "Now that we have our transcript ready, we are ready to generate the audio in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21c7e456-497b-4080-8b52-6f399f9f8d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df4d71-e660-4425-a8d1-0fad70fdd769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
